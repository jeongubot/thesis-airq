{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75aac7a0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from itertools import product\n",
    "from tqdm import tqdm \n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "118cc919",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class LSTMTemporalFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM component for hybrid models that produces 32-dimensional temporal features\n",
    "    Compatible with: CapsNet-LSTM-LightGBM, CNN-LSTM-LightGBM, CapsNet-LSTM\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=1, dropout=0.2, \n",
    "                 activation='relu', lstm_dropout=0.0):\n",
    "        super(LSTMTemporalFeatureExtractor, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers,\n",
    "            dropout=lstm_dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        else:\n",
    "            self.activation = nn.ReLU()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Feature extraction layer (32 dimensions for hybrid integration)\n",
    "        self.temporal_feature_layer = nn.Linear(hidden_size, 32)\n",
    "        \n",
    "        # Optional prediction layer (for CapsNet-LSTM model without LightGBM)\n",
    "        self.prediction_layer = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x, return_features_only=False):\n",
    "        \"\"\"\n",
    "        Forward pass with option to return only temporal features or full prediction\n",
    "        \n",
    "        Args:\n",
    "            x: Input sequences (batch_size, seq_len, features)\n",
    "            return_features_only: If True, returns only 32-dim temporal features\n",
    "                                If False, returns both features and prediction\n",
    "        \n",
    "        Returns:\n",
    "            temporal_features: 32-dimensional temporal features for hybrid models\n",
    "            prediction: PM2.5 prediction (only if return_features_only=False)\n",
    "        \"\"\"\n",
    "        # LSTM forward pass\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        last_output = lstm_out[:, -1, :]  # Use last timestep output\n",
    "        \n",
    "        # Extract 32-dimensional temporal features\n",
    "        temporal_features = self.activation(self.temporal_feature_layer(last_output))\n",
    "        temporal_features = self.dropout(temporal_features)\n",
    "        \n",
    "        if return_features_only:\n",
    "            return temporal_features\n",
    "        else:\n",
    "            # For CapsNet-LSTM model (without LightGBM)\n",
    "            prediction = self.prediction_layer(temporal_features)\n",
    "            return temporal_features, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54f1ce4c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class LSTMTemporalFeatureGenerator:\n",
    "    \"\"\"\n",
    "    LSTM component for temporal feature extraction in hybrid models\n",
    "    Designed to work within full pipeline cross-validation\n",
    "    \"\"\"\n",
    "    def __init__(self, best_params=None):\n",
    "        # Default hyperparameters (should be determined through separate tuning)\n",
    "        self.default_params = {\n",
    "            'hidden_size': 64,\n",
    "            'num_layers': 1,\n",
    "            'dropout': 0.2,\n",
    "            'activation': 'relu',\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 32,\n",
    "            'epochs': 50,\n",
    "            'timesteps': 60,  # Default temporal window size\n",
    "            'weight_decay': 0.0,\n",
    "            'grad_clip': 1.0,\n",
    "            'lstm_dropout': 0.0\n",
    "        }\n",
    "        \n",
    "        self.params = best_params if best_params else self.default_params\n",
    "        self.scaler = None\n",
    "        self.model = None\n",
    "        \n",
    "    def prepare_temporal_sequences(self, temporal_data, targets, timesteps=10):\n",
    "        \"\"\"\n",
    "        Prepare temporal sequences for a given fold\n",
    "        Called during each CV fold by the full pipeline\n",
    "        \"\"\"\n",
    "        # Scale temporal features\n",
    "        if self.scaler is None:\n",
    "            self.scaler = MinMaxScaler()\n",
    "            temporal_data_scaled = self.scaler.fit_transform(temporal_data)\n",
    "        else:\n",
    "            temporal_data_scaled = self.scaler.transform(temporal_data)\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = [], []\n",
    "        for i in range(len(temporal_data_scaled) - timesteps):\n",
    "            X.append(temporal_data_scaled[i:i+timesteps])\n",
    "            y.append(targets[i+timesteps])\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def train_and_extract_features(self, train_temporal_data, train_targets, val_temporal_data, val_targets, timesteps=None):\n",
    "        \"\"\"\n",
    "        Train LSTM and extract temporal features for current CV fold\n",
    "        This is called by the full pipeline during each fold\n",
    "        \n",
    "        IMPORTANT: val_temporal_data is validation data from the 80% learning set,\n",
    "                   NOT the 20% hold-out test set (to prevent data leakage)\n",
    "        \n",
    "        Returns:\n",
    "            train_features: 32D temporal features for training data\n",
    "            val_features: 32D temporal features for validation data (within learning set)\n",
    "        \"\"\"\n",
    "        # Use timesteps from params or default\n",
    "        if timesteps is None:\n",
    "            timesteps = self.params.get('timesteps', 60)\n",
    "            \n",
    "        # Prepare sequences for training\n",
    "        X_train, y_train = self.prepare_temporal_sequences(train_temporal_data, train_targets, timesteps)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "        \n",
    "        # Create data loader\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.params['batch_size'], shuffle=True)\n",
    "        \n",
    "        # Initialize LSTM model\n",
    "        self.model = LSTMTemporalFeatureExtractor(\n",
    "            input_size=train_temporal_data.shape[1],\n",
    "            hidden_size=self.params['hidden_size'],\n",
    "            num_layers=self.params['num_layers'],\n",
    "            dropout=self.params['dropout'],\n",
    "            activation=self.params['activation'],\n",
    "            lstm_dropout=self.params.get('lstm_dropout', 0.0)\n",
    "        )\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(\n",
    "            self.model.parameters(), \n",
    "            lr=self.params['learning_rate'],\n",
    "            weight_decay=self.params.get('weight_decay', 0.0)\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        self.model.train()\n",
    "        for epoch in range(self.params['epochs']):\n",
    "            total_loss = 0\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                _, predictions = self.model(batch_X, return_features_only=False)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(), \n",
    "                    max_norm=self.params.get('grad_clip', 1.0)\n",
    "                )\n",
    "                \n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        # Extract features from training data\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_features = self.model(X_train_tensor, return_features_only=True).numpy()\n",
    "        \n",
    "        # Prepare test sequences and extract features\n",
    "        X_val, _ = self.prepare_temporal_sequences(val_temporal_data, val_targets, timesteps)\n",
    "        X_val_tensor = torch.FloatTensor(X_val)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_features = self.model(X_val_tensor, return_features_only=True).numpy()\n",
    "        \n",
    "        # Debug: Verify numpy arrays are generated correctly\n",
    "        print(f\"  LSTM Debug - Train features shape: {train_features.shape}, type: {type(train_features)}\")\n",
    "        print(f\"  LSTM Debug - Val features shape: {val_features.shape}, type: {type(val_features)}\")\n",
    "        print(f\"  LSTM Debug - Feature sample: {train_features[0][:5]}...\")  # First 5 values\n",
    "        \n",
    "        return train_features, val_features, y_train, X_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2784e3eb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TemporalDataLoader:\n",
    "    \"\"\"\n",
    "    Data loader for temporal features across all days\n",
    "    Supports full pipeline cross-validation\n",
    "    \"\"\"\n",
    "    def __init__(self, days=['7_24', '10_19', '11_10']):\n",
    "        self.days = days\n",
    "        self.temporal_features = ['pm10', 'temperature', 'humidity']\n",
    "    \n",
    "    def load_temporal_data(self, day):\n",
    "        \"\"\"\n",
    "        Load temporal data for a specific day\n",
    "        Returns raw temporal data and targets for CV splitting\n",
    "        \"\"\"\n",
    "        matched_file = f'../../dataset/c_matched_spatio_temporal_data/matched_{day}.csv'\n",
    "        if not os.path.exists(matched_file):\n",
    "            raise FileNotFoundError(f\"Matched data not found: {matched_file}\")\n",
    "        \n",
    "        df = pd.read_csv(matched_file)\n",
    "        \n",
    "        # Sort by timestamp for proper time series handling\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "        # Get available temporal features\n",
    "        available_features = [col for col in self.temporal_features if col in df.columns]\n",
    "        \n",
    "        if not available_features:\n",
    "            raise ValueError(f\"No temporal features found in {day}\")\n",
    "        \n",
    "        # Remove rows with missing values\n",
    "        required_cols = available_features + ['pm2.5']\n",
    "        df_clean = df.dropna(subset=required_cols)\n",
    "        \n",
    "        temporal_data = df_clean[available_features].values\n",
    "        targets = df_clean['pm2.5'].values\n",
    "        \n",
    "        return temporal_data, targets, available_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c6f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning_lstm(day, n_trials=20):\n",
    "    \"\"\"\n",
    "    Hyperparameter tuning for LSTM component using Optuna.\n",
    "    \"\"\"\n",
    "    print(f\"LSTM Hyperparameter Tuning for {day}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Load data\n",
    "    data_loader = TemporalDataLoader()\n",
    "    temporal_data, targets, feature_names = data_loader.load_temporal_data(day)\n",
    "\n",
    "    n_total = len(temporal_data)\n",
    "    n_learning = int(n_total * 0.8)\n",
    "    learning_temporal = temporal_data[:n_learning]\n",
    "    learning_targets = targets[:n_learning]\n",
    "\n",
    "    n_tune_train = int(n_learning * 0.8)\n",
    "    tune_train_temporal = learning_temporal[:n_tune_train]\n",
    "    tune_train_targets = learning_targets[:n_tune_train]\n",
    "    tune_val_temporal = learning_temporal[n_tune_train:]\n",
    "    tune_val_targets = learning_targets[n_tune_train:]\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'hidden_size': trial.suggest_categorical('hidden_size', [32, 64, 128, 256]),\n",
    "            'num_layers': trial.suggest_int('num_layers', 1, 3),\n",
    "            'dropout': trial.suggest_float('dropout', 0.1, 0.2, 0.3, 0.4),\n",
    "            'activation': trial.suggest_categorical('activation', ['relu', 'tanh', 'gelu']),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 0.0001, 0.001, 0.01),\n",
    "            'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64]),\n",
    "            'epochs': trial.suggest_categorical('epochs', [10, 30, 50]),\n",
    "            'timesteps': trial.suggest_categorical('timesteps', [10, 30, 40, 60]),\n",
    "            'weight_decay': trial.suggest_categorical('weight_decay', [0.0, 1e-5, 1e-4]),\n",
    "            'grad_clip': trial.suggest_categorical('grad_clip', [0.5, 1.0, 2.0]),\n",
    "            'lstm_dropout': trial.suggest_categorical('lstm_dropout', [0.0, 0.1, 0.2])\n",
    "        }\n",
    "        lstm_gen = LSTMTemporalFeatureGenerator(params)\n",
    "        try:\n",
    "            train_features, val_features, train_y, val_len = lstm_gen.train_and_extract_features(\n",
    "                tune_train_temporal, tune_train_targets, tune_val_temporal, tune_val_targets,\n",
    "                timesteps=params['timesteps']\n",
    "            )\n",
    "            val_y = tune_val_targets[params['timesteps']:]\n",
    "            if len(val_y) == len(val_features):\n",
    "                from sklearn.linear_model import LinearRegression\n",
    "                lr = LinearRegression()\n",
    "                lr.fit(train_features, train_y)\n",
    "                val_pred = lr.predict(val_features)\n",
    "                mse = mean_squared_error(val_y, val_pred)\n",
    "                return mse\n",
    "            else:\n",
    "                return float('inf')\n",
    "        except Exception as e:\n",
    "            print(f\"Optuna trial error: {e}\")\n",
    "            return float('inf')\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_score = study.best_value\n",
    "\n",
    "    os.makedirs('models/lstm_temporal', exist_ok=True)\n",
    "    with open(f'models/lstm_temporal/{day}_best_params.json', 'w') as f:\n",
    "        json.dump(best_params, f, indent=2)\n",
    "\n",
    "    print(f\"Best parameters for {day}: {best_params}\")\n",
    "    print(f\"Best validation RMSE: {np.sqrt(best_score):.4f}\")\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddf4cb15",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution for LSTM temporal feature extraction\n",
    "    Focused on component preparation for full pipeline CV\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"LSTM TEMPORAL FEATURE EXTRACTION - PIPELINE READY\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Purpose: Prepare LSTM component for full pipeline cross-validation\")\n",
    "    print(\"- Hyperparameter tuning for each day\")\n",
    "    print(\"- Component ready for CV integration\")\n",
    "    print(\"- No standalone evaluation (done in full pipeline)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    days = ['7_24', '10_19', '11_10']\n",
    "    \n",
    "    # Step 1: Hyperparameter tuning for each day\n",
    "    print(\"\\nStep 1: LSTM Hyperparameter Tuning\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for day in days:\n",
    "        try:\n",
    "            print(f\"\\nTuning LSTM parameters for {day}...\")\n",
    "            best_params = hyperparameter_tuning_lstm(day, n_trials=20)\n",
    "            print(f\" {day} tuning complete\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Error tuning {day}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LSTM COMPONENT PREPARATION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Best parameters saved for each day\")\n",
    "    print(\"LSTMTemporalFeatureGenerator class ready\")\n",
    "    print(\"TemporalDataLoader class ready\")\n",
    "    print(\"\\nNext Steps:\")\n",
    "    print(\"1. Integration team: Use LSTMTemporalFeatureGenerator in full pipeline\")\n",
    "    print(\"2. Full pipeline CV: Wrap entire models in 5-fold cross-validation\")\n",
    "    print(\"3. Models to evaluate: CapsNet-LSTM-LightGBM, CNN-LSTM-LightGBM, CapsNet-LSTM\")\n",
    "    print(\"4. Statistical analysis: Compare 5-fold results across models\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Example usage for integration team\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CORRECT 5-FOLD CV INTEGRATION:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\"\"\n",
    "# PHASE 1: Hyperparameter Tuning (run once per day)\n",
    "from lstm_temporal_feature_generator import hyperparameter_tuning_lstm\n",
    "best_params = hyperparameter_tuning_lstm('7_24', max_combinations=10)\n",
    "\n",
    "# PHASE 2: 5-Fold Cross-Validation (main pipeline)\n",
    "from lstm_temporal_feature_generator import LSTMTemporalFeatureGenerator, TemporalDataLoader\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Initialize components\n",
    "data_loader = TemporalDataLoader()\n",
    "lstm_generator = LSTMTemporalFeatureGenerator(best_params)\n",
    "\n",
    "# Load data and split into learning (80%) and hold-out (20%)\n",
    "temporal_data, targets, _ = data_loader.load_temporal_data('7_24')\n",
    "n_total = len(temporal_data)\n",
    "n_learning = int(n_total * 0.8)\n",
    "\n",
    "learning_temporal = temporal_data[:n_learning]  # 80% learning set\n",
    "learning_targets = targets[:n_learning]\n",
    "holdout_temporal = temporal_data[n_learning:]   # 20% hold-out (untouched)\n",
    "holdout_targets = targets[n_learning:]\n",
    "\n",
    "# 5-Fold TimeSeriesSplit on 80% learning set\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "fold_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(learning_temporal)):\n",
    "    print(f\"Fold {fold + 1}/5\")\n",
    "    \n",
    "    # Split data for this fold (expanding window)\n",
    "    train_temporal = learning_temporal[train_idx]  # Growing train set\n",
    "    val_temporal = learning_temporal[val_idx]      # Fixed-size val set\n",
    "    train_targets = learning_targets[train_idx]\n",
    "    val_targets = learning_targets[val_idx]\n",
    "    \n",
    "    # Extract temporal features for this fold\n",
    "    train_temp_features, val_temp_features, _, _ = lstm_generator.train_and_extract_features(\n",
    "        train_temporal, train_targets, val_temporal, val_targets\n",
    "    )\n",
    "    \n",
    "    # Extract spatial features (CapsNet/CNN)\n",
    "    train_spatial_features, val_spatial_features = capsnet_generator.train_and_extract_features(\n",
    "        train_images, train_targets, val_images, val_targets\n",
    "    )\n",
    "    \n",
    "    # Combine features\n",
    "    train_combined = np.concatenate([train_temp_features, train_spatial_features], axis=1)\n",
    "    val_combined = np.concatenate([val_temp_features, val_spatial_features], axis=1)\n",
    "    \n",
    "    # Train LightGBM meta-learner\n",
    "    lightgbm_model = LGBMRegressor(best_lightgbm_params)\n",
    "    lightgbm_model.fit(train_combined, train_targets)\n",
    "    \n",
    "    # Validate on this fold\n",
    "    val_predictions = lightgbm_model.predict(val_combined)\n",
    "    fold_rmse = np.sqrt(mean_squared_error(val_targets, val_predictions))\n",
    "    fold_scores.append(fold_rmse)\n",
    "    \n",
    "    print(f\"  Fold {fold + 1} RMSE: {fold_rmse:.4f}\")\n",
    "\n",
    "# Calculate cross-validation performance\n",
    "cv_mean = np.mean(fold_scores)\n",
    "cv_std = np.std(fold_scores)\n",
    "print(f\"5-Fold CV Results: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "\"\"\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e84b21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-20 21:40:14,452] A new study created in memory with name: no-name-2469927d-d3fa-48e4-8393-a18f74de4271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LSTM TEMPORAL FEATURE EXTRACTION - PIPELINE READY\n",
      "================================================================================\n",
      "Purpose: Prepare LSTM component for full pipeline cross-validation\n",
      "- Hyperparameter tuning for each day\n",
      "- Component ready for CV integration\n",
      "- No standalone evaluation (done in full pipeline)\n",
      "================================================================================\n",
      "\n",
      "Step 1: LSTM Hyperparameter Tuning\n",
      "--------------------------------------------------\n",
      "\n",
      "Tuning LSTM parameters for 7_24...\n",
      "LSTM Hyperparameter Tuning for 7_24\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604e2a21589e49e3a453a7efee72f4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (41346, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10330, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.9999484  -0.9999732  -0.9998846   0.99989736  0.9998976 ]...\n",
      "[I 2025-07-20 21:43:56,738] Trial 0 finished with value: 27.931700504416252 and parameters: {'hidden_size': 256, 'num_layers': 2, 'dropout': 0.24825862760482284, 'activation': 'tanh', 'learning_rate': 0.004990872393387714, 'batch_size': 32, 'epochs': 10, 'timesteps': 10, 'weight_decay': 0.0001, 'grad_clip': 1.0, 'lstm_dropout': 0.1}. Best is trial 0 with value: 27.931700504416252.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (41316, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10300, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.532787    0.5655723  -0.5365905   0.5017635  -0.32525575]...\n",
      "[I 2025-07-20 22:10:39,568] Trial 1 finished with value: 27.781587308463614 and parameters: {'hidden_size': 256, 'num_layers': 1, 'dropout': 0.1625644375764122, 'activation': 'tanh', 'learning_rate': 0.000174740621040118, 'batch_size': 32, 'epochs': 50, 'timesteps': 40, 'weight_decay': 0.0, 'grad_clip': 0.5, 'lstm_dropout': 0.0}. Best is trial 1 with value: 27.781587308463614.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (41316, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10300, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 0.23215638  0.23297954 -0.2530873   0.24394113  0.19467811]...\n",
      "[I 2025-07-20 22:17:14,139] Trial 2 finished with value: 30.332522726196114 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.16197944380275936, 'activation': 'tanh', 'learning_rate': 0.005307987461054677, 'batch_size': 64, 'epochs': 30, 'timesteps': 40, 'weight_decay': 0.0001, 'grad_clip': 1.0, 'lstm_dropout': 0.1}. Best is trial 1 with value: 27.781587308463614.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (41326, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10310, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [3.459734  3.6476264 2.4506316 2.9092717 2.802512 ]...\n",
      "[I 2025-07-20 22:24:10,555] Trial 3 finished with value: 36.43886524050005 and parameters: {'hidden_size': 32, 'num_layers': 2, 'dropout': 0.15621609634952724, 'activation': 'relu', 'learning_rate': 0.00047961376598915534, 'batch_size': 32, 'epochs': 30, 'timesteps': 30, 'weight_decay': 0.0, 'grad_clip': 2.0, 'lstm_dropout': 0.2}. Best is trial 1 with value: 27.781587308463614.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (41326, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10310, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [0.        0.        1.2896491 0.        1.2323089]...\n",
      "[I 2025-07-20 22:37:52,146] Trial 4 finished with value: 37.04295588890999 and parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.30538749786380187, 'activation': 'relu', 'learning_rate': 0.0006104724852012556, 'batch_size': 32, 'epochs': 50, 'timesteps': 30, 'weight_decay': 0.0, 'grad_clip': 1.0, 'lstm_dropout': 0.0}. Best is trial 1 with value: 27.781587308463614.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (41326, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10310, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.43513814 -0.050433   -0.99179256  0.8921299  -0.97314376]...\n",
      "[I 2025-07-20 23:19:57,591] Trial 5 finished with value: 42.30087757851335 and parameters: {'hidden_size': 128, 'num_layers': 3, 'dropout': 0.36703084450987844, 'activation': 'tanh', 'learning_rate': 0.0023183479150438255, 'batch_size': 32, 'epochs': 50, 'timesteps': 30, 'weight_decay': 1e-05, 'grad_clip': 0.5, 'lstm_dropout': 0.2}. Best is trial 1 with value: 27.781587308463614.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (41346, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10330, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 0.6623071  -0.15031438 -0.14016235 -0.15714167 -0.15930524]...\n",
      "[I 2025-07-20 23:25:46,571] Trial 6 finished with value: 34.84665224726374 and parameters: {'hidden_size': 256, 'num_layers': 1, 'dropout': 0.3915039579424653, 'activation': 'gelu', 'learning_rate': 0.0006384689723116393, 'batch_size': 16, 'epochs': 30, 'timesteps': 10, 'weight_decay': 0.0001, 'grad_clip': 1.0, 'lstm_dropout': 0.1}. Best is trial 1 with value: 27.781587308463614.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (41296, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10280, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [1.7109332 0.        1.378523  1.4397821 0.       ]...\n",
      "[I 2025-07-20 23:29:13,790] Trial 7 finished with value: 27.442024484940397 and parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.24904618157403233, 'activation': 'relu', 'learning_rate': 0.0006165507072469879, 'batch_size': 32, 'epochs': 30, 'timesteps': 60, 'weight_decay': 0.0, 'grad_clip': 0.5, 'lstm_dropout': 0.2}. Best is trial 7 with value: 27.442024484940397.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (41296, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10280, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 0.99999416  0.9999995  -0.9999984  -0.9999242   0.99999976]...\n",
      "[I 2025-07-20 23:52:54,032] Trial 8 finished with value: 27.33516524486622 and parameters: {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.39362222854359386, 'activation': 'tanh', 'learning_rate': 0.00040439843490336266, 'batch_size': 32, 'epochs': 10, 'timesteps': 60, 'weight_decay': 0.0, 'grad_clip': 0.5, 'lstm_dropout': 0.1}. Best is trial 8 with value: 27.33516524486622.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (41296, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10280, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 2.0821428  -0.1699417  -0.16960336  2.083252   -0.16996965]...\n",
      "[I 2025-07-21 00:09:43,301] Trial 9 finished with value: 33.660629492362474 and parameters: {'hidden_size': 64, 'num_layers': 3, 'dropout': 0.22742265989462473, 'activation': 'gelu', 'learning_rate': 0.00016012923707732631, 'batch_size': 16, 'epochs': 30, 'timesteps': 60, 'weight_decay': 0.0001, 'grad_clip': 0.5, 'lstm_dropout': 0.1}. Best is trial 8 with value: 27.33516524486622.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (41296, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10280, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.9957112  -0.9839221   0.98153263 -0.9880384   0.9861653 ]...\n",
      "[I 2025-07-21 00:11:37,363] Trial 10 finished with value: 43.55297534654669 and parameters: {'hidden_size': 32, 'num_layers': 3, 'dropout': 0.3306938818334361, 'activation': 'tanh', 'learning_rate': 0.0017770447761885763, 'batch_size': 64, 'epochs': 10, 'timesteps': 60, 'weight_decay': 1e-05, 'grad_clip': 2.0, 'lstm_dropout': 0.1}. Best is trial 8 with value: 27.33516524486622.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (41296, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10280, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [0. 0. 0. 0. 0.]...\n",
      "[I 2025-07-21 00:14:16,778] Trial 11 finished with value: 34.02895912959291 and parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.2838980888456125, 'activation': 'relu', 'learning_rate': 0.0002735713552096249, 'batch_size': 32, 'epochs': 10, 'timesteps': 60, 'weight_decay': 0.0, 'grad_clip': 0.5, 'lstm_dropout': 0.2}. Best is trial 8 with value: 27.33516524486622.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (41296, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10280, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [2.0132709 0.        1.793696  2.007837  2.0295784]...\n",
      "[I 2025-07-21 00:15:27,524] Trial 12 finished with value: 32.540958250724664 and parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.21243893283473625, 'activation': 'relu', 'learning_rate': 0.0013086659102409864, 'batch_size': 32, 'epochs': 10, 'timesteps': 60, 'weight_decay': 0.0, 'grad_clip': 0.5, 'lstm_dropout': 0.2}. Best is trial 8 with value: 27.33516524486622.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (41296, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10280, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [7.243396 0.       0.       0.       6.292638]...\n",
      "[I 2025-07-21 01:41:43,648] Trial 13 finished with value: 32.8588419758404 and parameters: {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.10593545229019105, 'activation': 'relu', 'learning_rate': 0.00033157364368797607, 'batch_size': 64, 'epochs': 30, 'timesteps': 60, 'weight_decay': 0.0, 'grad_clip': 0.5, 'lstm_dropout': 0.2}. Best is trial 8 with value: 27.33516524486622.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (41296, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10280, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 4.1491838   4.982781    4.397761   -0.11286008 -0.16977699]...\n",
      "[I 2025-07-21 02:02:31,793] Trial 14 finished with value: 38.05210856330205 and parameters: {'hidden_size': 256, 'num_layers': 2, 'dropout': 0.2796606958668747, 'activation': 'gelu', 'learning_rate': 0.000914103698749428, 'batch_size': 32, 'epochs': 10, 'timesteps': 60, 'weight_decay': 0.0, 'grad_clip': 0.5, 'lstm_dropout': 0.0}. Best is trial 8 with value: 27.33516524486622.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (41296, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10280, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.39827484 -0.39070684 -0.43401685 -0.39898676 -0.44104162]...\n",
      "[I 2025-07-21 02:13:41,046] Trial 15 finished with value: 32.01594893561809 and parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.3424061047951089, 'activation': 'tanh', 'learning_rate': 0.0002862615986732835, 'batch_size': 16, 'epochs': 30, 'timesteps': 60, 'weight_decay': 0.0, 'grad_clip': 0.5, 'lstm_dropout': 0.1}. Best is trial 8 with value: 27.33516524486622.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (41296, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10280, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [4.7541575 5.460004  5.7692413 0.        0.       ]...\n",
      "[I 2025-07-21 02:22:13,329] Trial 16 finished with value: 32.66314827020818 and parameters: {'hidden_size': 128, 'num_layers': 3, 'dropout': 0.2055610449626293, 'activation': 'relu', 'learning_rate': 0.00010079392539744863, 'batch_size': 32, 'epochs': 10, 'timesteps': 60, 'weight_decay': 1e-05, 'grad_clip': 2.0, 'lstm_dropout': 0.2}. Best is trial 8 with value: 27.33516524486622.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (41346, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10330, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 0.46256256 -0.17318773  0.49364138 -0.37554586  0.4324383 ]...\n",
      "[I 2025-07-21 02:22:59,249] Trial 17 finished with value: 34.0472598755481 and parameters: {'hidden_size': 32, 'num_layers': 2, 'dropout': 0.3920626204471143, 'activation': 'tanh', 'learning_rate': 0.0023623525808321236, 'batch_size': 32, 'epochs': 10, 'timesteps': 10, 'weight_decay': 0.0, 'grad_clip': 0.5, 'lstm_dropout': 0.1}. Best is trial 8 with value: 27.33516524486622.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (41316, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10300, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [0. 0. 0. 0. 0.]...\n",
      "[I 2025-07-21 12:51:24,932] Trial 18 finished with value: 27.744520330698414 and parameters: {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.26875015283320963, 'activation': 'relu', 'learning_rate': 0.009436485120621502, 'batch_size': 64, 'epochs': 30, 'timesteps': 40, 'weight_decay': 0.0, 'grad_clip': 0.5, 'lstm_dropout': 0.2}. Best is trial 8 with value: 27.33516524486622.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (41296, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10280, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.09192718  0.05786441 -0.07157504  0.15026644 -0.0912802 ]...\n",
      "[I 2025-07-21 13:00:17,936] Trial 19 finished with value: 33.55156220446712 and parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.33066074397871553, 'activation': 'gelu', 'learning_rate': 0.001001661186988162, 'batch_size': 16, 'epochs': 50, 'timesteps': 60, 'weight_decay': 1e-05, 'grad_clip': 2.0, 'lstm_dropout': 0.0}. Best is trial 8 with value: 27.33516524486622.\n",
      "Best parameters for 7_24: {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.39362222854359386, 'activation': 'tanh', 'learning_rate': 0.00040439843490336266, 'batch_size': 32, 'epochs': 10, 'timesteps': 60, 'weight_decay': 0.0, 'grad_clip': 0.5, 'lstm_dropout': 0.1}\n",
      "Best validation RMSE: 5.2283\n",
      " 7_24 tuning complete\n",
      "\n",
      "Tuning LSTM parameters for 10_19...\n",
      "LSTM Hyperparameter Tuning for 10_19\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-21 13:00:18,490] A new study created in memory with name: no-name-b82a3a87-ad7b-4f60-bfc4-18069d1dc79e\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489e206bdf7f4436b5dd1c28d426a626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (79208, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19757, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.87896407  0.86469793  0.85259837 -0.88333374 -0.87272924]...\n",
      "[I 2025-07-21 13:52:48,898] Trial 0 finished with value: 45.178733589937636 and parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.3802901620631436, 'activation': 'tanh', 'learning_rate': 0.00035606760221285077, 'batch_size': 16, 'epochs': 30, 'timesteps': 60, 'weight_decay': 0.0001, 'grad_clip': 1.0, 'lstm_dropout': 0.1}. Best is trial 0 with value: 45.178733589937636.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (79258, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19807, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 0.9998746 -0.9998711 -0.9998178  0.9996819  0.9997524]...\n",
      "[I 2025-07-21 15:15:15,515] Trial 1 finished with value: 87.42861981247785 and parameters: {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.12357491844550512, 'activation': 'tanh', 'learning_rate': 0.0016567218750908282, 'batch_size': 64, 'epochs': 30, 'timesteps': 10, 'weight_decay': 0.0001, 'grad_clip': 1.0, 'lstm_dropout': 0.0}. Best is trial 0 with value: 45.178733589937636.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (79258, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19807, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 0.20415491  0.18454538  0.2150461  -0.1846387   0.2279506 ]...\n",
      "[I 2025-07-21 15:41:02,684] Trial 2 finished with value: 49.14972700665197 and parameters: {'hidden_size': 256, 'num_layers': 1, 'dropout': 0.3605639341265797, 'activation': 'tanh', 'learning_rate': 0.0005315253477015005, 'batch_size': 32, 'epochs': 50, 'timesteps': 10, 'weight_decay': 0.0, 'grad_clip': 2.0, 'lstm_dropout': 0.2}. Best is trial 0 with value: 45.178733589937636.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (79238, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19787, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 2.4640687  -0.15881956  1.9915441   2.8513205  -0.16736428]...\n",
      "[I 2025-07-21 15:51:50,038] Trial 3 finished with value: 47.57757611105058 and parameters: {'hidden_size': 32, 'num_layers': 2, 'dropout': 0.1261923690304398, 'activation': 'gelu', 'learning_rate': 0.0018453412867667834, 'batch_size': 64, 'epochs': 30, 'timesteps': 30, 'weight_decay': 0.0, 'grad_clip': 0.5, 'lstm_dropout': 0.1}. Best is trial 0 with value: 45.178733589937636.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (79208, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19757, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 4.0146585  -0.1699429   4.028287    4.410253   -0.16994147]...\n",
      "[I 2025-07-21 16:12:26,676] Trial 4 finished with value: 44.35473092818972 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.2507464297862117, 'activation': 'gelu', 'learning_rate': 0.0004063409215497555, 'batch_size': 16, 'epochs': 30, 'timesteps': 60, 'weight_decay': 0.0, 'grad_clip': 2.0, 'lstm_dropout': 0.0}. Best is trial 4 with value: 44.35473092818972.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_12076\\350409549.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2),\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
