{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75aac7a0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from itertools import product\n",
    "from tqdm import tqdm \n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "118cc919",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class LSTMTemporalFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM component for hybrid models that produces 32-dimensional temporal features\n",
    "    Compatible with: CapsNet-LSTM-LightGBM, CNN-LSTM-LightGBM, CapsNet-LSTM\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=1, dropout=0.2, \n",
    "                 activation='relu', lstm_dropout=0.0):\n",
    "        super(LSTMTemporalFeatureExtractor, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers,\n",
    "            dropout=lstm_dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        else:\n",
    "            self.activation = nn.ReLU()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Feature extraction layer (32 dimensions for hybrid integration)\n",
    "        self.temporal_feature_layer = nn.Linear(hidden_size, 32)\n",
    "        \n",
    "        # Optional prediction layer (for CapsNet-LSTM model without LightGBM)\n",
    "        self.prediction_layer = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x, return_features_only=False):\n",
    "        \"\"\"\n",
    "        Forward pass with option to return only temporal features or full prediction\n",
    "        \n",
    "        Args:\n",
    "            x: Input sequences (batch_size, seq_len, features)\n",
    "            return_features_only: If True, returns only 32-dim temporal features\n",
    "                                If False, returns both features and prediction\n",
    "        \n",
    "        Returns:\n",
    "            temporal_features: 32-dimensional temporal features for hybrid models\n",
    "            prediction: PM2.5 prediction (only if return_features_only=False)\n",
    "        \"\"\"\n",
    "        # LSTM forward pass\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        last_output = lstm_out[:, -1, :]  # Use last timestep output\n",
    "        \n",
    "        # Extract 32-dimensional temporal features\n",
    "        temporal_features = self.activation(self.temporal_feature_layer(last_output))\n",
    "        temporal_features = self.dropout(temporal_features)\n",
    "        \n",
    "        if return_features_only:\n",
    "            return temporal_features\n",
    "        else:\n",
    "            # For CapsNet-LSTM model (without LightGBM)\n",
    "            prediction = self.prediction_layer(temporal_features)\n",
    "            return temporal_features, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54f1ce4c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class LSTMTemporalFeatureGenerator:\n",
    "    \"\"\"\n",
    "    LSTM component for temporal feature extraction in hybrid models\n",
    "    Designed to work within full pipeline cross-validation\n",
    "    \"\"\"\n",
    "    def __init__(self, best_params=None):\n",
    "        # Default hyperparameters (should be determined through separate tuning)\n",
    "        self.default_params = {\n",
    "            'hidden_size': 64,\n",
    "            'num_layers': 1,\n",
    "            'dropout': 0.2,\n",
    "            'activation': 'relu',\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 32,\n",
    "            'epochs': 50,\n",
    "            'timesteps': 60,  # Default temporal window size\n",
    "            'weight_decay': 0.0,\n",
    "            'grad_clip': 1.0,\n",
    "            'lstm_dropout': 0.0\n",
    "        }\n",
    "        \n",
    "        self.params = best_params if best_params else self.default_params\n",
    "        self.scaler = None\n",
    "        self.model = None\n",
    "        \n",
    "    def prepare_temporal_sequences(self, temporal_data, targets, timesteps=10):\n",
    "        \"\"\"\n",
    "        Prepare temporal sequences for a given fold\n",
    "        Called during each CV fold by the full pipeline\n",
    "        \"\"\"\n",
    "        # Scale temporal features\n",
    "        if self.scaler is None:\n",
    "            self.scaler = MinMaxScaler()\n",
    "            temporal_data_scaled = self.scaler.fit_transform(temporal_data)\n",
    "        else:\n",
    "            temporal_data_scaled = self.scaler.transform(temporal_data)\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = [], []\n",
    "        for i in range(len(temporal_data_scaled) - timesteps):\n",
    "            X.append(temporal_data_scaled[i:i+timesteps])\n",
    "            y.append(targets[i+timesteps])\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def train_and_extract_features(self, train_temporal_data, train_targets, val_temporal_data, val_targets, timesteps=None):\n",
    "        \"\"\"\n",
    "        Train LSTM and extract temporal features for current CV fold\n",
    "        This is called by the full pipeline during each fold\n",
    "        \n",
    "        IMPORTANT: val_temporal_data is validation data from the 80% learning set,\n",
    "                   NOT the 20% hold-out test set (to prevent data leakage)\n",
    "        \n",
    "        Returns:\n",
    "            train_features: 32D temporal features for training data\n",
    "            val_features: 32D temporal features for validation data (within learning set)\n",
    "        \"\"\"\n",
    "        # Use timesteps from params or default\n",
    "        if timesteps is None:\n",
    "            timesteps = self.params.get('timesteps', 60)\n",
    "            \n",
    "        # Prepare sequences for training\n",
    "        X_train, y_train = self.prepare_temporal_sequences(train_temporal_data, train_targets, timesteps)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "        \n",
    "        # Create data loader\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.params['batch_size'], shuffle=True)\n",
    "        \n",
    "        # Initialize LSTM model\n",
    "        self.model = LSTMTemporalFeatureExtractor(\n",
    "            input_size=train_temporal_data.shape[1],\n",
    "            hidden_size=self.params['hidden_size'],\n",
    "            num_layers=self.params['num_layers'],\n",
    "            dropout=self.params['dropout'],\n",
    "            activation=self.params['activation'],\n",
    "            lstm_dropout=self.params.get('lstm_dropout', 0.0)\n",
    "        )\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(\n",
    "            self.model.parameters(), \n",
    "            lr=self.params['learning_rate'],\n",
    "            weight_decay=self.params.get('weight_decay', 0.0)\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        self.model.train()\n",
    "        for epoch in range(self.params['epochs']):\n",
    "            total_loss = 0\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                _, predictions = self.model(batch_X, return_features_only=False)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(), \n",
    "                    max_norm=self.params.get('grad_clip', 1.0)\n",
    "                )\n",
    "                \n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        # Extract features from training data\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_features = self.model(X_train_tensor, return_features_only=True).numpy()\n",
    "        \n",
    "        # Prepare test sequences and extract features\n",
    "        X_val, _ = self.prepare_temporal_sequences(val_temporal_data, val_targets, timesteps)\n",
    "        X_val_tensor = torch.FloatTensor(X_val)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_features = self.model(X_val_tensor, return_features_only=True).numpy()\n",
    "        \n",
    "        # Debug: Verify numpy arrays are generated correctly\n",
    "        print(f\"  LSTM Debug - Train features shape: {train_features.shape}, type: {type(train_features)}\")\n",
    "        print(f\"  LSTM Debug - Val features shape: {val_features.shape}, type: {type(val_features)}\")\n",
    "        print(f\"  LSTM Debug - Feature sample: {train_features[0][:5]}...\")  # First 5 values\n",
    "        \n",
    "        return train_features, val_features, y_train, X_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2784e3eb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TemporalDataLoader:\n",
    "    \"\"\"\n",
    "    Data loader for temporal features across all days\n",
    "    Supports full pipeline cross-validation\n",
    "    \"\"\"\n",
    "    def __init__(self, days=['7_24', '10_19', '11_10']):\n",
    "        self.days = days\n",
    "        self.temporal_features = ['pm10', 'temperature', 'humidity']\n",
    "    \n",
    "    def load_temporal_data(self, day):\n",
    "        \"\"\"\n",
    "        Load temporal data for a specific day\n",
    "        Returns raw temporal data and targets for CV splitting\n",
    "        \"\"\"\n",
    "        matched_file = f'../../dataset/c_matched_spatio_temporal_data/matched_{day}.csv'\n",
    "        if not os.path.exists(matched_file):\n",
    "            raise FileNotFoundError(f\"Matched data not found: {matched_file}\")\n",
    "        \n",
    "        df = pd.read_csv(matched_file)\n",
    "        \n",
    "        # Sort by timestamp for proper time series handling\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "        # Get available temporal features\n",
    "        available_features = [col for col in self.temporal_features if col in df.columns]\n",
    "        \n",
    "        if not available_features:\n",
    "            raise ValueError(f\"No temporal features found in {day}\")\n",
    "        \n",
    "        # Remove rows with missing values\n",
    "        required_cols = available_features + ['pm2.5']\n",
    "        df_clean = df.dropna(subset=required_cols)\n",
    "        \n",
    "        temporal_data = df_clean[available_features].values\n",
    "        targets = df_clean['pm2.5'].values\n",
    "        \n",
    "        return temporal_data, targets, available_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c6f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning_lstm(day, n_trials=20):\n",
    "    \"\"\"\n",
    "    Hyperparameter tuning for LSTM component using Optuna.\n",
    "    \"\"\"\n",
    "    print(f\"LSTM Hyperparameter Tuning for {day}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Load data\n",
    "    data_loader = TemporalDataLoader()\n",
    "    temporal_data, targets, feature_names = data_loader.load_temporal_data(day)\n",
    "\n",
    "    n_total = len(temporal_data)\n",
    "    n_learning = int(n_total * 0.8)\n",
    "    learning_temporal = temporal_data[:n_learning]\n",
    "    learning_targets = targets[:n_learning]\n",
    "\n",
    "    n_tune_train = int(n_learning * 0.8)\n",
    "    tune_train_temporal = learning_temporal[:n_tune_train]\n",
    "    tune_train_targets = learning_targets[:n_tune_train]\n",
    "    tune_val_temporal = learning_temporal[n_tune_train:]\n",
    "    tune_val_targets = learning_targets[n_tune_train:]\n",
    " \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'hidden_size': trial.suggest_categorical('hidden_size', [32, 64, 128, 256]),\n",
    "            'num_layers': trial.suggest_int('num_layers', 1, 3),\n",
    "            'dropout': trial.suggest_categorical('dropout', [0.1, 0.2, 0.3, 0.4]),\n",
    "            'activation': trial.suggest_categorical('activation', ['relu', 'tanh', 'gelu']),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.01, log=True),\n",
    "            'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64]),\n",
    "            'epochs': trial.suggest_categorical('epochs', [10, 30, 50]),\n",
    "            'timesteps': trial.suggest_categorical('timesteps', [10, 30, 40, 60]),\n",
    "            'weight_decay': trial.suggest_categorical('weight_decay', [0.0, 1e-5, 1e-4]),\n",
    "            'grad_clip': trial.suggest_categorical('grad_clip', [0.5, 1.0, 2.0]),\n",
    "            'lstm_dropout': trial.suggest_categorical('lstm_dropout', [0.0, 0.1, 0.2])\n",
    "        }\n",
    "        lstm_gen = LSTMTemporalFeatureGenerator(params)\n",
    "        try:\n",
    "            train_features, val_features, train_y, val_len = lstm_gen.train_and_extract_features(\n",
    "                tune_train_temporal, tune_train_targets, tune_val_temporal, tune_val_targets,\n",
    "                timesteps=params['timesteps']\n",
    "            )\n",
    "            val_y = tune_val_targets[params['timesteps']:]\n",
    "            if len(val_y) == len(val_features):\n",
    "                from sklearn.linear_model import LinearRegression\n",
    "                lr = LinearRegression()\n",
    "                lr.fit(train_features, train_y)\n",
    "                val_pred = lr.predict(val_features)\n",
    "                mse = mean_squared_error(val_y, val_pred)\n",
    "                return mse\n",
    "            else:\n",
    "                return float('inf')\n",
    "        except Exception as e:\n",
    "            print(f\"Optuna trial error: {e}\")\n",
    "            return float('inf')\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_score = study.best_value\n",
    "\n",
    "    os.makedirs('models/lstm_temporal', exist_ok=True)\n",
    "    with open(f'models/lstm_temporal/{day}_best_params.json', 'w') as f:\n",
    "        json.dump(best_params, f, indent=2)\n",
    "\n",
    "    print(f\"Best parameters for {day}: {best_params}\")\n",
    "    print(f\"Best validation RMSE: {np.sqrt(best_score):.4f}\")\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddf4cb15",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution for LSTM temporal feature extraction\n",
    "    Focused on component preparation for full pipeline CV\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"LSTM TEMPORAL FEATURE EXTRACTION - PIPELINE READY\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Purpose: Prepare LSTM component for full pipeline cross-validation\")\n",
    "    print(\"- Hyperparameter tuning for each day\")\n",
    "    print(\"- Component ready for CV integration\")\n",
    "    print(\"- No standalone evaluation (done in full pipeline)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    days = ['7_24', '10_19', '11_10']\n",
    "    \n",
    "    # Step 1: Hyperparameter tuning for each day\n",
    "    print(\"\\nStep 1: LSTM Hyperparameter Tuning\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for day in days:\n",
    "        try:\n",
    "            print(f\"\\nTuning LSTM parameters for {day}...\")\n",
    "            best_params = hyperparameter_tuning_lstm(day, n_trials=20)\n",
    "            print(f\" {day} tuning complete\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Error tuning {day}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LSTM COMPONENT PREPARATION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Best parameters saved for each day\")\n",
    "    print(\"LSTMTemporalFeatureGenerator class ready\")\n",
    "    print(\"TemporalDataLoader class ready\")\n",
    "    print(\"\\nNext Steps:\")\n",
    "    print(\"1. Integration team: Use LSTMTemporalFeatureGenerator in full pipeline\")\n",
    "    print(\"2. Full pipeline CV: Wrap entire models in 5-fold cross-validation\")\n",
    "    print(\"3. Models to evaluate: CapsNet-LSTM-LightGBM, CNN-LSTM-LightGBM, CapsNet-LSTM\")\n",
    "    print(\"4. Statistical analysis: Compare 5-fold results across models\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Example usage for integration team\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CORRECT 5-FOLD CV INTEGRATION:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\"\"\n",
    "# PHASE 1: Hyperparameter Tuning (run once per day)\n",
    "from lstm_temporal_feature_generator import hyperparameter_tuning_lstm\n",
    "best_params = hyperparameter_tuning_lstm('7_24', max_combinations=10)\n",
    "\n",
    "# PHASE 2: 5-Fold Cross-Validation (main pipeline)\n",
    "from lstm_temporal_feature_generator import LSTMTemporalFeatureGenerator, TemporalDataLoader\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Initialize components\n",
    "data_loader = TemporalDataLoader()\n",
    "lstm_generator = LSTMTemporalFeatureGenerator(best_params)\n",
    "\n",
    "# Load data and split into learning (80%) and hold-out (20%)\n",
    "temporal_data, targets, _ = data_loader.load_temporal_data('7_24')\n",
    "n_total = len(temporal_data)\n",
    "n_learning = int(n_total * 0.8)\n",
    "\n",
    "learning_temporal = temporal_data[:n_learning]  # 80% learning set\n",
    "learning_targets = targets[:n_learning]\n",
    "holdout_temporal = temporal_data[n_learning:]   # 20% hold-out (untouched)\n",
    "holdout_targets = targets[n_learning:]\n",
    "\n",
    "# 5-Fold TimeSeriesSplit on 80% learning set\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "fold_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(learning_temporal)):\n",
    "    print(f\"Fold {fold + 1}/5\")\n",
    "    \n",
    "    # Split data for this fold (expanding window)\n",
    "    train_temporal = learning_temporal[train_idx]  # Growing train set\n",
    "    val_temporal = learning_temporal[val_idx]      # Fixed-size val set\n",
    "    train_targets = learning_targets[train_idx]\n",
    "    val_targets = learning_targets[val_idx]\n",
    "    \n",
    "    # Extract temporal features for this fold\n",
    "    train_temp_features, val_temp_features, _, _ = lstm_generator.train_and_extract_features(\n",
    "        train_temporal, train_targets, val_temporal, val_targets\n",
    "    )\n",
    "    \n",
    "    # Extract spatial features (CapsNet/CNN)\n",
    "    train_spatial_features, val_spatial_features = capsnet_generator.train_and_extract_features(\n",
    "        train_images, train_targets, val_images, val_targets\n",
    "    )\n",
    "    \n",
    "    # Combine features\n",
    "    train_combined = np.concatenate([train_temp_features, train_spatial_features], axis=1)\n",
    "    val_combined = np.concatenate([val_temp_features, val_spatial_features], axis=1)\n",
    "    \n",
    "    # Train LightGBM meta-learner\n",
    "    lightgbm_model = LGBMRegressor(best_lightgbm_params)\n",
    "    lightgbm_model.fit(train_combined, train_targets)\n",
    "    \n",
    "    # Validate on this fold\n",
    "    val_predictions = lightgbm_model.predict(val_combined)\n",
    "    fold_rmse = np.sqrt(mean_squared_error(val_targets, val_predictions))\n",
    "    fold_scores.append(fold_rmse)\n",
    "    \n",
    "    print(f\"  Fold {fold + 1} RMSE: {fold_rmse:.4f}\")\n",
    "\n",
    "# Calculate cross-validation performance\n",
    "cv_mean = np.mean(fold_scores)\n",
    "cv_std = np.std(fold_scores)\n",
    "print(f\"5-Fold CV Results: {cv_mean:.4f} Â± {cv_std:.4f}\")\n",
    "\"\"\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e84b21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-21 16:36:28,635] A new study created in memory with name: no-name-b9896faf-1783-4ab1-9ca3-67f195ec4049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LSTM TEMPORAL FEATURE EXTRACTION - PIPELINE READY\n",
      "================================================================================\n",
      "Purpose: Prepare LSTM component for full pipeline cross-validation\n",
      "- Hyperparameter tuning for each day\n",
      "- Component ready for CV integration\n",
      "- No standalone evaluation (done in full pipeline)\n",
      "================================================================================\n",
      "\n",
      "Step 1: LSTM Hyperparameter Tuning\n",
      "--------------------------------------------------\n",
      "\n",
      "Tuning LSTM parameters for 7_24...\n",
      "LSTM Hyperparameter Tuning for 7_24\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f37e48bf3414cb9a461fa236beb6c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (41346, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10330, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 0.78712326 -0.3081217   0.52941567 -0.05849542 -0.18633407]...\n",
      "[I 2025-07-21 16:49:47,015] Trial 0 finished with value: 39.19637106132161 and parameters: {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.2, 'activation': 'tanh', 'learning_rate': 0.0017569709690772235, 'batch_size': 64, 'epochs': 30, 'timesteps': 10, 'weight_decay': 1e-05, 'grad_clip': 2.0, 'lstm_dropout': 0.2}. Best is trial 0 with value: 39.19637106132161.\n",
      "  LSTM Debug - Train features shape: (41296, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10280, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.52805007  0.507845    0.44838214 -0.48477915  0.48258948]...\n",
      "[I 2025-07-21 17:18:21,956] Trial 1 finished with value: 33.93604810693149 and parameters: {'hidden_size': 256, 'num_layers': 1, 'dropout': 0.2, 'activation': 'tanh', 'learning_rate': 0.00014568862349842644, 'batch_size': 32, 'epochs': 50, 'timesteps': 60, 'weight_decay': 0.0001, 'grad_clip': 1.0, 'lstm_dropout': 0.0}. Best is trial 1 with value: 33.93604810693149.\n",
      "  LSTM Debug - Train features shape: (41316, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10300, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 0.26909882  0.26140454 -0.21880151  0.25116667 -0.31987357]...\n",
      "[I 2025-07-21 17:51:42,439] Trial 2 finished with value: 27.74532486994669 and parameters: {'hidden_size': 64, 'num_layers': 3, 'dropout': 0.1, 'activation': 'tanh', 'learning_rate': 0.0024002419574533267, 'batch_size': 16, 'epochs': 50, 'timesteps': 40, 'weight_decay': 1e-05, 'grad_clip': 0.5, 'lstm_dropout': 0.1}. Best is trial 2 with value: 27.74532486994669.\n",
      "  LSTM Debug - Train features shape: (41296, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10280, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 0.99851966 -0.9080561   0.9980538   0.99986225 -0.9988094 ]...\n",
      "[I 2025-07-21 18:06:01,703] Trial 3 finished with value: 42.070052460785504 and parameters: {'hidden_size': 64, 'num_layers': 3, 'dropout': 0.3, 'activation': 'tanh', 'learning_rate': 0.006650827311952762, 'batch_size': 64, 'epochs': 30, 'timesteps': 60, 'weight_decay': 1e-05, 'grad_clip': 2.0, 'lstm_dropout': 0.0}. Best is trial 2 with value: 27.74532486994669.\n",
      "  LSTM Debug - Train features shape: (41296, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10280, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.16110365 -0.16861068  0.23193856  0.27595672  0.37354246]...\n",
      "[I 2025-07-21 18:09:33,415] Trial 4 finished with value: 28.520134988323328 and parameters: {'hidden_size': 32, 'num_layers': 1, 'dropout': 0.1, 'activation': 'gelu', 'learning_rate': 0.005425385974801939, 'batch_size': 64, 'epochs': 30, 'timesteps': 60, 'weight_decay': 1e-05, 'grad_clip': 0.5, 'lstm_dropout': 0.2}. Best is trial 2 with value: 27.74532486994669.\n",
      "  LSTM Debug - Train features shape: (41296, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10280, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [5.8075714 6.128221  6.032408  3.071983  4.010291 ]...\n",
      "[I 2025-07-21 22:57:53,925] Trial 5 finished with value: 28.408536289834455 and parameters: {'hidden_size': 256, 'num_layers': 2, 'dropout': 0.2, 'activation': 'relu', 'learning_rate': 0.0001572772353719598, 'batch_size': 64, 'epochs': 50, 'timesteps': 60, 'weight_decay': 1e-05, 'grad_clip': 1.0, 'lstm_dropout': 0.2}. Best is trial 2 with value: 27.74532486994669.\n",
      "  LSTM Debug - Train features shape: (41296, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10280, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 0.36547095  0.16669595  0.9274291   0.9675903  -0.8692847 ]...\n",
      "[I 2025-07-21 23:14:09,694] Trial 6 finished with value: 37.370713334550004 and parameters: {'hidden_size': 256, 'num_layers': 1, 'dropout': 0.1, 'activation': 'tanh', 'learning_rate': 0.006214574123137868, 'batch_size': 32, 'epochs': 30, 'timesteps': 60, 'weight_decay': 1e-05, 'grad_clip': 0.5, 'lstm_dropout': 0.2}. Best is trial 2 with value: 27.74532486994669.\n",
      "  LSTM Debug - Train features shape: (41346, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10330, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.14878216  1.4344635   2.0796225  -0.10890324  2.0382087 ]...\n",
      "[I 2025-07-21 23:21:32,555] Trial 7 finished with value: 33.33986411498281 and parameters: {'hidden_size': 256, 'num_layers': 1, 'dropout': 0.3, 'activation': 'gelu', 'learning_rate': 0.00047465766738967707, 'batch_size': 32, 'epochs': 50, 'timesteps': 10, 'weight_decay': 0.0, 'grad_clip': 1.0, 'lstm_dropout': 0.1}. Best is trial 2 with value: 27.74532486994669.\n",
      "  LSTM Debug - Train features shape: (41346, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10330, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 0.509434   -0.446179    0.52475536  0.60956657 -0.5736699 ]...\n",
      "[I 2025-07-21 23:23:22,377] Trial 8 finished with value: 34.445477036202625 and parameters: {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.3, 'activation': 'tanh', 'learning_rate': 0.00024082002420552253, 'batch_size': 64, 'epochs': 50, 'timesteps': 10, 'weight_decay': 0.0, 'grad_clip': 2.0, 'lstm_dropout': 0.2}. Best is trial 2 with value: 27.74532486994669.\n",
      "  LSTM Debug - Train features shape: (41316, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10300, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.23551491 -0.3226196   0.2687024   0.2966227   0.2698227 ]...\n",
      "[I 2025-07-21 23:52:23,488] Trial 9 finished with value: 30.696912507398977 and parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.4, 'activation': 'tanh', 'learning_rate': 0.0003501443940048907, 'batch_size': 16, 'epochs': 50, 'timesteps': 40, 'weight_decay': 1e-05, 'grad_clip': 0.5, 'lstm_dropout': 0.0}. Best is trial 2 with value: 27.74532486994669.\n",
      "  LSTM Debug - Train features shape: (41316, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10300, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [1.1008236 1.3616455 1.2816336 1.4031163 1.2426007]...\n",
      "[I 2025-07-22 00:00:58,286] Trial 10 finished with value: 35.01552391519165 and parameters: {'hidden_size': 64, 'num_layers': 3, 'dropout': 0.1, 'activation': 'relu', 'learning_rate': 0.0018126559022151206, 'batch_size': 16, 'epochs': 10, 'timesteps': 40, 'weight_decay': 0.0001, 'grad_clip': 0.5, 'lstm_dropout': 0.1}. Best is trial 2 with value: 27.74532486994669.\n",
      "  LSTM Debug - Train features shape: (41326, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10310, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [1.3876445 1.5371675 1.4023008 1.464058  1.5520079]...\n",
      "[I 2025-07-22 00:37:49,089] Trial 11 finished with value: 34.81303708973623 and parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.2, 'activation': 'relu', 'learning_rate': 0.0008314317724307499, 'batch_size': 16, 'epochs': 50, 'timesteps': 30, 'weight_decay': 1e-05, 'grad_clip': 1.0, 'lstm_dropout': 0.1}. Best is trial 2 with value: 27.74532486994669.\n",
      "  LSTM Debug - Train features shape: (41316, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10300, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [2.8775141 2.646672  2.5096748 2.9778085 2.4972193]...\n",
      "[I 2025-07-22 00:41:44,418] Trial 12 finished with value: 33.75105405134718 and parameters: {'hidden_size': 32, 'num_layers': 2, 'dropout': 0.4, 'activation': 'relu', 'learning_rate': 0.00010132569111447733, 'batch_size': 16, 'epochs': 10, 'timesteps': 40, 'weight_decay': 1e-05, 'grad_clip': 1.0, 'lstm_dropout': 0.1}. Best is trial 2 with value: 27.74532486994669.\n",
      "  LSTM Debug - Train features shape: (41326, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10310, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [1.563592 0.       0.       0.       0.      ]...\n",
      "[I 2025-07-22 00:59:12,117] Trial 13 finished with value: 30.058351324676867 and parameters: {'hidden_size': 64, 'num_layers': 3, 'dropout': 0.1, 'activation': 'relu', 'learning_rate': 0.0026687051727846095, 'batch_size': 64, 'epochs': 50, 'timesteps': 30, 'weight_decay': 1e-05, 'grad_clip': 1.0, 'lstm_dropout': 0.2}. Best is trial 2 with value: 27.74532486994669.\n",
      "  LSTM Debug - Train features shape: (41316, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10300, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [3.9943295 4.341115  5.6363363 7.229103  4.316332 ]...\n",
      "[I 2025-07-22 02:27:40,342] Trial 14 finished with value: 27.74453343944622 and parameters: {'hidden_size': 256, 'num_layers': 2, 'dropout': 0.2, 'activation': 'relu', 'learning_rate': 0.000987781242922646, 'batch_size': 16, 'epochs': 50, 'timesteps': 40, 'weight_decay': 0.0001, 'grad_clip': 0.5, 'lstm_dropout': 0.1}. Best is trial 14 with value: 27.74453343944622.\n",
      "  LSTM Debug - Train features shape: (41316, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10300, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.1581259   0.64612645  0.5598211   0.6680201  -0.16261096]...\n",
      "[I 2025-07-22 02:42:42,419] Trial 15 finished with value: 36.41821528882249 and parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.2, 'activation': 'gelu', 'learning_rate': 0.0007887791907461212, 'batch_size': 16, 'epochs': 50, 'timesteps': 40, 'weight_decay': 0.0001, 'grad_clip': 0.5, 'lstm_dropout': 0.1}. Best is trial 14 with value: 27.74453343944622.\n",
      "  LSTM Debug - Train features shape: (41316, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10300, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [3.8221638 4.3415756 6.411397  0.        3.5748096]...\n",
      "[I 2025-07-22 02:50:43,754] Trial 16 finished with value: 27.744521002672677 and parameters: {'hidden_size': 128, 'num_layers': 3, 'dropout': 0.1, 'activation': 'relu', 'learning_rate': 0.00292202448787691, 'batch_size': 16, 'epochs': 10, 'timesteps': 40, 'weight_decay': 0.0001, 'grad_clip': 0.5, 'lstm_dropout': 0.1}. Best is trial 16 with value: 27.744521002672677.\n",
      "  LSTM Debug - Train features shape: (41316, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10300, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [1.4801216 0.        0.        1.3532125 1.2854311]...\n",
      "[I 2025-07-22 02:55:56,577] Trial 17 finished with value: 36.081667335175254 and parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.4, 'activation': 'relu', 'learning_rate': 0.0012410276337433, 'batch_size': 16, 'epochs': 10, 'timesteps': 40, 'weight_decay': 0.0001, 'grad_clip': 0.5, 'lstm_dropout': 0.1}. Best is trial 16 with value: 27.744521002672677.\n",
      "  LSTM Debug - Train features shape: (41316, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10300, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [0.        0.        0.        3.2993681 2.2844572]...\n",
      "[I 2025-07-22 03:03:57,268] Trial 18 finished with value: 41.58730225652306 and parameters: {'hidden_size': 128, 'num_layers': 3, 'dropout': 0.1, 'activation': 'relu', 'learning_rate': 0.004145747916585737, 'batch_size': 16, 'epochs': 10, 'timesteps': 40, 'weight_decay': 0.0001, 'grad_clip': 0.5, 'lstm_dropout': 0.1}. Best is trial 16 with value: 27.744521002672677.\n",
      "  LSTM Debug - Train features shape: (41316, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (10300, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [1.8382008 1.5671887 0.        2.2081358 2.3297412]...\n",
      "[I 2025-07-22 03:09:08,205] Trial 19 finished with value: 41.648875759443264 and parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.2, 'activation': 'relu', 'learning_rate': 0.0034708263349052635, 'batch_size': 16, 'epochs': 10, 'timesteps': 40, 'weight_decay': 0.0001, 'grad_clip': 0.5, 'lstm_dropout': 0.1}. Best is trial 16 with value: 27.744521002672677.\n",
      "Best parameters for 7_24: {'hidden_size': 128, 'num_layers': 3, 'dropout': 0.1, 'activation': 'relu', 'learning_rate': 0.00292202448787691, 'batch_size': 16, 'epochs': 10, 'timesteps': 40, 'weight_decay': 0.0001, 'grad_clip': 0.5, 'lstm_dropout': 0.1}\n",
      "Best validation RMSE: 5.2673\n",
      " 7_24 tuning complete\n",
      "\n",
      "Tuning LSTM parameters for 10_19...\n",
      "LSTM Hyperparameter Tuning for 10_19\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 03:09:08,652] A new study created in memory with name: no-name-71f73d9d-9787-4641-a48d-4efaef8081f7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e63efec1e643578d62307d8ae2ae85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (79228, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19777, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 0.8525432   1.9185919   1.7398748  -0.16997117  1.7182778 ]...\n",
      "[I 2025-07-22 03:31:59,387] Trial 0 finished with value: 46.832022053708606 and parameters: {'hidden_size': 256, 'num_layers': 1, 'dropout': 0.2, 'activation': 'gelu', 'learning_rate': 0.005579013653719452, 'batch_size': 32, 'epochs': 30, 'timesteps': 40, 'weight_decay': 0.0001, 'grad_clip': 2.0, 'lstm_dropout': 0.1}. Best is trial 0 with value: 46.832022053708606.\n",
      "  LSTM Debug - Train features shape: (79258, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19807, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.07190542 -0.06239362 -0.16739719  7.705181   -0.16698237]...\n",
      "[I 2025-07-22 03:36:05,903] Trial 1 finished with value: 51.02283623420228 and parameters: {'hidden_size': 256, 'num_layers': 1, 'dropout': 0.4, 'activation': 'gelu', 'learning_rate': 0.0007417010886520028, 'batch_size': 16, 'epochs': 10, 'timesteps': 10, 'weight_decay': 0.0, 'grad_clip': 0.5, 'lstm_dropout': 0.2}. Best is trial 0 with value: 46.832022053708606.\n",
      "  LSTM Debug - Train features shape: (79228, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19777, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [4.231721  4.120392  4.033942  4.3545027 4.3247976]...\n",
      "[I 2025-07-22 03:40:49,524] Trial 2 finished with value: 46.574157182295615 and parameters: {'hidden_size': 32, 'num_layers': 1, 'dropout': 0.2, 'activation': 'relu', 'learning_rate': 0.0004721058750328882, 'batch_size': 64, 'epochs': 50, 'timesteps': 40, 'weight_decay': 1e-05, 'grad_clip': 0.5, 'lstm_dropout': 0.2}. Best is trial 2 with value: 46.574157182295615.\n",
      "  LSTM Debug - Train features shape: (79258, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19807, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [4.308852  4.8665853 6.0830903 0.        3.929318 ]...\n",
      "[I 2025-07-22 03:42:08,814] Trial 3 finished with value: 50.81150843604231 and parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.4, 'activation': 'relu', 'learning_rate': 0.0021850331321696757, 'batch_size': 64, 'epochs': 10, 'timesteps': 10, 'weight_decay': 0.0001, 'grad_clip': 2.0, 'lstm_dropout': 0.2}. Best is trial 2 with value: 46.574157182295615.\n",
      "  LSTM Debug - Train features shape: (79228, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19777, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [11.592304   12.346928   13.368916   -0.16993411 14.096509  ]...\n",
      "[I 2025-07-22 03:52:54,054] Trial 4 finished with value: 52.92780036470179 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.2, 'activation': 'gelu', 'learning_rate': 0.0001314862951332001, 'batch_size': 64, 'epochs': 30, 'timesteps': 40, 'weight_decay': 0.0001, 'grad_clip': 2.0, 'lstm_dropout': 0.1}. Best is trial 2 with value: 46.574157182295615.\n",
      "  LSTM Debug - Train features shape: (79258, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19807, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.6852565  -0.6740814  -0.650075    0.6753079  -0.63657784]...\n",
      "[I 2025-07-22 15:04:45,451] Trial 5 finished with value: 49.70887506333107 and parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.2, 'activation': 'tanh', 'learning_rate': 0.0001308297826610673, 'batch_size': 16, 'epochs': 30, 'timesteps': 10, 'weight_decay': 0.0, 'grad_clip': 2.0, 'lstm_dropout': 0.0}. Best is trial 2 with value: 46.574157182295615.\n",
      "  LSTM Debug - Train features shape: (79228, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19777, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [2.0480316 1.9836309 1.8249131 2.2559824 1.8058109]...\n",
      "[I 2025-07-22 15:12:26,280] Trial 6 finished with value: 46.3935178892948 and parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.3, 'activation': 'relu', 'learning_rate': 0.0021811343517232924, 'batch_size': 64, 'epochs': 30, 'timesteps': 40, 'weight_decay': 0.0001, 'grad_clip': 0.5, 'lstm_dropout': 0.0}. Best is trial 6 with value: 46.3935178892948.\n",
      "  LSTM Debug - Train features shape: (79208, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19757, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 3.7829697  -0.1682548  -0.16996677 -0.16981511 -0.16996029]...\n",
      "[I 2025-07-22 15:15:43,833] Trial 7 finished with value: 45.62478923154409 and parameters: {'hidden_size': 32, 'num_layers': 2, 'dropout': 0.4, 'activation': 'gelu', 'learning_rate': 0.0014923931611783659, 'batch_size': 32, 'epochs': 10, 'timesteps': 60, 'weight_decay': 0.0, 'grad_clip': 0.5, 'lstm_dropout': 0.0}. Best is trial 7 with value: 45.62478923154409.\n",
      "Optuna trial error: [enforce fail at alloc_cpu.cpp:116] data. DefaultCPUAllocator: not enough memory: you tried to allocate 43514992024 bytes.\n",
      "[I 2025-07-22 18:49:50,533] Trial 8 finished with value: inf and parameters: {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.3, 'activation': 'relu', 'learning_rate': 0.00010160527036851387, 'batch_size': 32, 'epochs': 30, 'timesteps': 60, 'weight_decay': 0.0001, 'grad_clip': 2.0, 'lstm_dropout': 0.2}. Best is trial 7 with value: 45.62478923154409.\n",
      "  LSTM Debug - Train features shape: (79208, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19757, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [2.95406   3.172278  2.923724  2.8601148 2.7314074]...\n",
      "[I 2025-07-22 18:58:08,015] Trial 9 finished with value: 44.29075827317288 and parameters: {'hidden_size': 32, 'num_layers': 1, 'dropout': 0.3, 'activation': 'relu', 'learning_rate': 0.0003619405533980962, 'batch_size': 16, 'epochs': 30, 'timesteps': 60, 'weight_decay': 0.0001, 'grad_clip': 2.0, 'lstm_dropout': 0.0}. Best is trial 9 with value: 44.29075827317288.\n",
      "  LSTM Debug - Train features shape: (79238, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19787, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 0.5647222  -0.6542165  -0.67343473  0.6482757  -0.6418918 ]...\n",
      "[I 2025-07-22 19:20:22,706] Trial 10 finished with value: 46.95592785767816 and parameters: {'hidden_size': 32, 'num_layers': 3, 'dropout': 0.1, 'activation': 'tanh', 'learning_rate': 0.00029889271396043064, 'batch_size': 16, 'epochs': 50, 'timesteps': 30, 'weight_decay': 1e-05, 'grad_clip': 1.0, 'lstm_dropout': 0.0}. Best is trial 9 with value: 44.29075827317288.\n",
      "  LSTM Debug - Train features shape: (79208, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19757, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.16968845 -0.16740665  5.2012296   5.914349    5.569927  ]...\n",
      "[I 2025-07-22 19:23:28,668] Trial 11 finished with value: 45.46951211451997 and parameters: {'hidden_size': 32, 'num_layers': 2, 'dropout': 0.3, 'activation': 'gelu', 'learning_rate': 0.0011766410209607667, 'batch_size': 32, 'epochs': 10, 'timesteps': 60, 'weight_decay': 0.0, 'grad_clip': 1.0, 'lstm_dropout': 0.0}. Best is trial 9 with value: 44.29075827317288.\n",
      "  LSTM Debug - Train features shape: (79208, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19757, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 6.8020673  -0.16987278 -0.16981919 -0.16995268 -0.16987255]...\n",
      "[I 2025-07-22 19:29:18,564] Trial 12 finished with value: 46.94155140274351 and parameters: {'hidden_size': 32, 'num_layers': 3, 'dropout': 0.3, 'activation': 'gelu', 'learning_rate': 0.000311837606103364, 'batch_size': 16, 'epochs': 10, 'timesteps': 60, 'weight_decay': 0.0, 'grad_clip': 1.0, 'lstm_dropout': 0.0}. Best is trial 9 with value: 44.29075827317288.\n",
      "  LSTM Debug - Train features shape: (79208, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19757, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 0.        7.740809  0.       10.179148  0.      ]...\n",
      "[I 2025-07-22 19:31:07,976] Trial 13 finished with value: 46.66751034600391 and parameters: {'hidden_size': 32, 'num_layers': 1, 'dropout': 0.3, 'activation': 'relu', 'learning_rate': 0.0008644145798783918, 'batch_size': 32, 'epochs': 10, 'timesteps': 60, 'weight_decay': 0.0, 'grad_clip': 1.0, 'lstm_dropout': 0.0}. Best is trial 9 with value: 44.29075827317288.\n",
      "  LSTM Debug - Train features shape: (79208, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19757, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 0.4230042   0.46210685  0.42932925 -0.58657634 -0.43634364]...\n",
      "[I 2025-07-22 20:21:09,306] Trial 14 finished with value: 45.01211842029994 and parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.3, 'activation': 'tanh', 'learning_rate': 0.00041729953812640915, 'batch_size': 32, 'epochs': 50, 'timesteps': 60, 'weight_decay': 1e-05, 'grad_clip': 1.0, 'lstm_dropout': 0.0}. Best is trial 9 with value: 44.29075827317288.\n",
      "  LSTM Debug - Train features shape: (79238, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19787, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 0.75535077 -0.72942775 -0.7370132   0.7464078  -0.73188823]...\n",
      "[I 2025-07-22 20:39:18,238] Trial 15 finished with value: 47.227250518315415 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.1, 'activation': 'tanh', 'learning_rate': 0.00027069056521271005, 'batch_size': 16, 'epochs': 50, 'timesteps': 30, 'weight_decay': 1e-05, 'grad_clip': 1.0, 'lstm_dropout': 0.0}. Best is trial 9 with value: 44.29075827317288.\n",
      "  LSTM Debug - Train features shape: (79208, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19757, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.3301791  -0.3969944   0.39095774 -0.41901615 -0.38289487]...\n",
      "[I 2025-07-22 22:18:28,808] Trial 16 finished with value: 45.304736946393255 and parameters: {'hidden_size': 128, 'num_layers': 3, 'dropout': 0.3, 'activation': 'tanh', 'learning_rate': 0.0005007358689801895, 'batch_size': 16, 'epochs': 50, 'timesteps': 60, 'weight_decay': 1e-05, 'grad_clip': 1.0, 'lstm_dropout': 0.1}. Best is trial 9 with value: 44.29075827317288.\n",
      "  LSTM Debug - Train features shape: (79208, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19757, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 0.6562139   0.63967264 -0.66726154  0.6080176  -0.63220453]...\n",
      "[I 2025-07-22 23:04:19,110] Trial 17 finished with value: 43.9039071162771 and parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.3, 'activation': 'tanh', 'learning_rate': 0.0002081261455101455, 'batch_size': 32, 'epochs': 50, 'timesteps': 60, 'weight_decay': 1e-05, 'grad_clip': 2.0, 'lstm_dropout': 0.0}. Best is trial 17 with value: 43.9039071162771.\n",
      "  LSTM Debug - Train features shape: (79208, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19757, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.5054039  -0.44699863  0.5030865   0.47571743  0.48011553]...\n",
      "[I 2025-07-22 23:32:08,676] Trial 18 finished with value: 44.985444083843014 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.3, 'activation': 'tanh', 'learning_rate': 0.00021753048189849839, 'batch_size': 16, 'epochs': 50, 'timesteps': 60, 'weight_decay': 1e-05, 'grad_clip': 2.0, 'lstm_dropout': 0.0}. Best is trial 17 with value: 43.9039071162771.\n",
      "  LSTM Debug - Train features shape: (79238, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19787, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [8.840744  8.323662  6.7767997 7.0956197 9.288192 ]...\n",
      "[I 2025-07-22 23:50:21,745] Trial 19 finished with value: 47.21850928759388 and parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.1, 'activation': 'relu', 'learning_rate': 0.0001844935277312219, 'batch_size': 32, 'epochs': 30, 'timesteps': 30, 'weight_decay': 0.0001, 'grad_clip': 2.0, 'lstm_dropout': 0.1}. Best is trial 17 with value: 43.9039071162771.\n",
      "Best parameters for 10_19: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.3, 'activation': 'tanh', 'learning_rate': 0.0002081261455101455, 'batch_size': 32, 'epochs': 50, 'timesteps': 60, 'weight_decay': 1e-05, 'grad_clip': 2.0, 'lstm_dropout': 0.0}\n",
      "Best validation RMSE: 6.6260\n",
      " 10_19 tuning complete\n",
      "\n",
      "Tuning LSTM parameters for 11_10...\n",
      "LSTM Hyperparameter Tuning for 11_10\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 23:50:22,129] A new study created in memory with name: no-name-7b739549-c024-4848-ace8-465e9d261597\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d64f68abe5ce4173af21655ca55b815b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LSTM Debug - Train features shape: (76223, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19034, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.7857156  -0.8115556   0.7143823  -0.76299894  0.7586211 ]...\n",
      "[I 2025-07-22 23:56:16,067] Trial 0 finished with value: 31.13441308841305 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.4, 'activation': 'tanh', 'learning_rate': 0.00013449555592120752, 'batch_size': 64, 'epochs': 30, 'timesteps': 30, 'weight_decay': 1e-05, 'grad_clip': 0.5, 'lstm_dropout': 0.2}. Best is trial 0 with value: 31.13441308841305.\n",
      "  LSTM Debug - Train features shape: (76243, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19054, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [0.         0.         0.01252955 0.         0.        ]...\n",
      "[I 2025-07-23 00:02:00,447] Trial 1 finished with value: 47.24470532416137 and parameters: {'hidden_size': 32, 'num_layers': 3, 'dropout': 0.3, 'activation': 'relu', 'learning_rate': 0.0013313582939375869, 'batch_size': 32, 'epochs': 30, 'timesteps': 10, 'weight_decay': 0.0, 'grad_clip': 0.5, 'lstm_dropout': 0.2}. Best is trial 0 with value: 31.13441308841305.\n",
      "  LSTM Debug - Train features shape: (76243, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19054, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 1.003881   -0.0831233  -0.09024011 -0.08771212 -0.10350319]...\n",
      "[I 2025-07-23 01:05:00,796] Trial 2 finished with value: 51.85057926824496 and parameters: {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.2, 'activation': 'gelu', 'learning_rate': 0.0009209317938106364, 'batch_size': 16, 'epochs': 30, 'timesteps': 10, 'weight_decay': 1e-05, 'grad_clip': 0.5, 'lstm_dropout': 0.0}. Best is trial 0 with value: 31.13441308841305.\n",
      "  LSTM Debug - Train features shape: (76243, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19054, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [0. 0. 0. 0. 0.]...\n",
      "[I 2025-07-23 01:22:56,136] Trial 3 finished with value: 43.83831889004859 and parameters: {'hidden_size': 128, 'num_layers': 3, 'dropout': 0.4, 'activation': 'relu', 'learning_rate': 0.002205177081987769, 'batch_size': 64, 'epochs': 30, 'timesteps': 10, 'weight_decay': 0.0, 'grad_clip': 1.0, 'lstm_dropout': 0.2}. Best is trial 0 with value: 31.13441308841305.\n",
      "Optuna trial error: [enforce fail at alloc_cpu.cpp:116] data. DefaultCPUAllocator: not enough memory: you tried to allocate 28797558680 bytes.\n",
      "[I 2025-07-23 01:48:55,184] Trial 4 finished with value: inf and parameters: {'hidden_size': 256, 'num_layers': 1, 'dropout': 0.3, 'activation': 'gelu', 'learning_rate': 0.0016548650287073552, 'batch_size': 16, 'epochs': 30, 'timesteps': 40, 'weight_decay': 1e-05, 'grad_clip': 0.5, 'lstm_dropout': 0.2}. Best is trial 0 with value: 31.13441308841305.\n",
      "  LSTM Debug - Train features shape: (76193, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19004, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 0.68537325 -0.5867006   0.639107    0.64246017 -0.60516256]...\n",
      "[I 2025-07-23 01:50:02,950] Trial 5 finished with value: 35.78071885440043 and parameters: {'hidden_size': 32, 'num_layers': 1, 'dropout': 0.1, 'activation': 'tanh', 'learning_rate': 0.006809125384599985, 'batch_size': 64, 'epochs': 10, 'timesteps': 60, 'weight_decay': 0.0, 'grad_clip': 2.0, 'lstm_dropout': 0.1}. Best is trial 0 with value: 31.13441308841305.\n",
      "  LSTM Debug - Train features shape: (76243, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19054, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [5.607848 5.627185 4.459798 4.835557 5.122205]...\n",
      "[I 2025-07-23 02:08:44,848] Trial 6 finished with value: 35.63584602923167 and parameters: {'hidden_size': 256, 'num_layers': 1, 'dropout': 0.3, 'activation': 'relu', 'learning_rate': 0.00015383842286992056, 'batch_size': 16, 'epochs': 50, 'timesteps': 10, 'weight_decay': 0.0001, 'grad_clip': 1.0, 'lstm_dropout': 0.2}. Best is trial 0 with value: 31.13441308841305.\n",
      "  LSTM Debug - Train features shape: (76193, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19004, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [0.        0.        6.1319885 0.        6.8378296]...\n",
      "[I 2025-07-23 02:27:03,216] Trial 7 finished with value: 41.68150031610591 and parameters: {'hidden_size': 32, 'num_layers': 3, 'dropout': 0.2, 'activation': 'relu', 'learning_rate': 0.00026337171048652783, 'batch_size': 16, 'epochs': 30, 'timesteps': 60, 'weight_decay': 0.0, 'grad_clip': 2.0, 'lstm_dropout': 0.1}. Best is trial 0 with value: 31.13441308841305.\n",
      "  LSTM Debug - Train features shape: (76193, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19004, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 1.5472119  -0.16837183  1.4060366   1.8896208   1.4092453 ]...\n",
      "[I 2025-07-23 02:38:02,443] Trial 8 finished with value: 33.811021558275336 and parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.3, 'activation': 'gelu', 'learning_rate': 0.0034631418363182936, 'batch_size': 32, 'epochs': 10, 'timesteps': 60, 'weight_decay': 0.0001, 'grad_clip': 1.0, 'lstm_dropout': 0.2}. Best is trial 0 with value: 31.13441308841305.\n",
      "  LSTM Debug - Train features shape: (76243, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19054, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.10959469 -0.16020213 -0.16980869 -0.1632972  -0.16856274]...\n",
      "[I 2025-07-23 13:38:31,416] Trial 9 finished with value: 47.71425579462504 and parameters: {'hidden_size': 256, 'num_layers': 2, 'dropout': 0.2, 'activation': 'gelu', 'learning_rate': 0.009216896327411935, 'batch_size': 64, 'epochs': 30, 'timesteps': 10, 'weight_decay': 1e-05, 'grad_clip': 0.5, 'lstm_dropout': 0.2}. Best is trial 0 with value: 31.13441308841305.\n",
      "  LSTM Debug - Train features shape: (76223, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19034, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.5503027  -0.5820233   0.54871005 -0.57242745 -0.48099232]...\n",
      "[I 2025-07-23 14:08:17,441] Trial 10 finished with value: 35.2687993272572 and parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.4, 'activation': 'tanh', 'learning_rate': 0.00043133173841582704, 'batch_size': 64, 'epochs': 50, 'timesteps': 30, 'weight_decay': 1e-05, 'grad_clip': 0.5, 'lstm_dropout': 0.0}. Best is trial 0 with value: 31.13441308841305.\n",
      "  LSTM Debug - Train features shape: (76223, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19034, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.2800922   0.24006541  0.18593317 -0.38832238 -0.33054492]...\n",
      "[I 2025-07-23 14:14:53,067] Trial 11 finished with value: 42.24412419926385 and parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.4, 'activation': 'tanh', 'learning_rate': 0.0037970819319604943, 'batch_size': 32, 'epochs': 10, 'timesteps': 30, 'weight_decay': 0.0001, 'grad_clip': 1.0, 'lstm_dropout': 0.2}. Best is trial 0 with value: 31.13441308841305.\n",
      "  LSTM Debug - Train features shape: (76223, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19034, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [14.200818   -0.16997094 -0.1699583  -0.16997094 -0.1699643 ]...\n",
      "[I 2025-07-23 14:21:45,550] Trial 12 finished with value: 35.867023538955095 and parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.1, 'activation': 'gelu', 'learning_rate': 0.00010180276574115648, 'batch_size': 32, 'epochs': 10, 'timesteps': 30, 'weight_decay': 0.0001, 'grad_clip': 1.0, 'lstm_dropout': 0.2}. Best is trial 0 with value: 31.13441308841305.\n",
      "  LSTM Debug - Train features shape: (76193, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19004, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 0.6715436 -0.7664593  0.7171886  0.6775508 -0.7278535]...\n",
      "[I 2025-07-23 14:26:18,387] Trial 13 finished with value: 61.2995399661155 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.3, 'activation': 'tanh', 'learning_rate': 0.000563389561081089, 'batch_size': 32, 'epochs': 10, 'timesteps': 60, 'weight_decay': 0.0001, 'grad_clip': 1.0, 'lstm_dropout': 0.0}. Best is trial 0 with value: 31.13441308841305.\n",
      "  LSTM Debug - Train features shape: (76213, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19024, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 3.9822059   3.4003735  -0.13523638  4.130134   -0.1699676 ]...\n",
      "[I 2025-07-23 14:29:04,686] Trial 14 finished with value: 46.95808069255527 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.4, 'activation': 'gelu', 'learning_rate': 0.003308823808452264, 'batch_size': 64, 'epochs': 10, 'timesteps': 40, 'weight_decay': 0.0001, 'grad_clip': 2.0, 'lstm_dropout': 0.2}. Best is trial 0 with value: 31.13441308841305.\n",
      "  LSTM Debug - Train features shape: (76223, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19034, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.24701835  0.31691045  0.29822588 -0.30773795  0.21252409]...\n",
      "[I 2025-07-23 14:45:58,898] Trial 15 finished with value: 48.761534185169964 and parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.3, 'activation': 'tanh', 'learning_rate': 0.0007363486832243647, 'batch_size': 32, 'epochs': 50, 'timesteps': 30, 'weight_decay': 1e-05, 'grad_clip': 0.5, 'lstm_dropout': 0.1}. Best is trial 0 with value: 31.13441308841305.\n",
      "  LSTM Debug - Train features shape: (76193, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19004, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 0.95158744  0.9363799   0.8762229  -0.91549236 -0.9481946 ]...\n",
      "[I 2025-07-23 14:49:46,581] Trial 16 finished with value: 42.31582630571341 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.4, 'activation': 'tanh', 'learning_rate': 0.000330104405309497, 'batch_size': 64, 'epochs': 10, 'timesteps': 60, 'weight_decay': 0.0001, 'grad_clip': 1.0, 'lstm_dropout': 0.2}. Best is trial 0 with value: 31.13441308841305.\n",
      "  LSTM Debug - Train features shape: (76223, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19034, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [ 3.5301986  3.4427752  3.7345977  3.592585  -0.0827788]...\n",
      "[I 2025-07-23 14:57:00,472] Trial 17 finished with value: 29.089116707061372 and parameters: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.1, 'activation': 'gelu', 'learning_rate': 0.004896671782774286, 'batch_size': 32, 'epochs': 10, 'timesteps': 30, 'weight_decay': 1e-05, 'grad_clip': 0.5, 'lstm_dropout': 0.2}. Best is trial 17 with value: 29.089116707061372.\n",
      "  LSTM Debug - Train features shape: (76223, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19034, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.69339174  0.72224    -0.7571323   0.7355988   0.7205479 ]...\n",
      "[I 2025-07-23 15:03:07,481] Trial 18 finished with value: 47.67877298739336 and parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.1, 'activation': 'tanh', 'learning_rate': 0.00024796575384293664, 'batch_size': 64, 'epochs': 30, 'timesteps': 30, 'weight_decay': 1e-05, 'grad_clip': 0.5, 'lstm_dropout': 0.1}. Best is trial 17 with value: 29.089116707061372.\n",
      "  LSTM Debug - Train features shape: (76223, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Val features shape: (19034, 32), type: <class 'numpy.ndarray'>\n",
      "  LSTM Debug - Feature sample: [-0.12739287 -0.14242136 -0.10256777 -0.13287815 -0.07557736]...\n",
      "[I 2025-07-23 15:17:52,287] Trial 19 finished with value: 56.59980646240237 and parameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.1, 'activation': 'gelu', 'learning_rate': 0.005386578850265354, 'batch_size': 32, 'epochs': 50, 'timesteps': 30, 'weight_decay': 1e-05, 'grad_clip': 0.5, 'lstm_dropout': 0.0}. Best is trial 17 with value: 29.089116707061372.\n",
      "Best parameters for 11_10: {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.1, 'activation': 'gelu', 'learning_rate': 0.004896671782774286, 'batch_size': 32, 'epochs': 10, 'timesteps': 30, 'weight_decay': 1e-05, 'grad_clip': 0.5, 'lstm_dropout': 0.2}\n",
      "Best validation RMSE: 5.3934\n",
      " 11_10 tuning complete\n",
      "\n",
      "================================================================================\n",
      "LSTM COMPONENT PREPARATION COMPLETE\n",
      "================================================================================\n",
      "Best parameters saved for each day\n",
      "LSTMTemporalFeatureGenerator class ready\n",
      "TemporalDataLoader class ready\n",
      "\n",
      "Next Steps:\n",
      "1. Integration team: Use LSTMTemporalFeatureGenerator in full pipeline\n",
      "2. Full pipeline CV: Wrap entire models in 5-fold cross-validation\n",
      "3. Models to evaluate: CapsNet-LSTM-LightGBM, CNN-LSTM-LightGBM, CapsNet-LSTM\n",
      "4. Statistical analysis: Compare 5-fold results across models\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "CORRECT 5-FOLD CV INTEGRATION:\n",
      "================================================================================\n",
      "\n",
      "# PHASE 1: Hyperparameter Tuning (run once per day)\n",
      "from lstm_temporal_feature_generator import hyperparameter_tuning_lstm\n",
      "best_params = hyperparameter_tuning_lstm('7_24', max_combinations=10)\n",
      "\n",
      "# PHASE 2: 5-Fold Cross-Validation (main pipeline)\n",
      "from lstm_temporal_feature_generator import LSTMTemporalFeatureGenerator, TemporalDataLoader\n",
      "from sklearn.model_selection import TimeSeriesSplit\n",
      "\n",
      "# Initialize components\n",
      "data_loader = TemporalDataLoader()\n",
      "lstm_generator = LSTMTemporalFeatureGenerator(best_params)\n",
      "\n",
      "# Load data and split into learning (80%) and hold-out (20%)\n",
      "temporal_data, targets, _ = data_loader.load_temporal_data('7_24')\n",
      "n_total = len(temporal_data)\n",
      "n_learning = int(n_total * 0.8)\n",
      "\n",
      "learning_temporal = temporal_data[:n_learning]  # 80% learning set\n",
      "learning_targets = targets[:n_learning]\n",
      "holdout_temporal = temporal_data[n_learning:]   # 20% hold-out (untouched)\n",
      "holdout_targets = targets[n_learning:]\n",
      "\n",
      "# 5-Fold TimeSeriesSplit on 80% learning set\n",
      "tscv = TimeSeriesSplit(n_splits=5)\n",
      "fold_scores = []\n",
      "\n",
      "for fold, (train_idx, val_idx) in enumerate(tscv.split(learning_temporal)):\n",
      "    print(f\"Fold {fold + 1}/5\")\n",
      "    \n",
      "    # Split data for this fold (expanding window)\n",
      "    train_temporal = learning_temporal[train_idx]  # Growing train set\n",
      "    val_temporal = learning_temporal[val_idx]      # Fixed-size val set\n",
      "    train_targets = learning_targets[train_idx]\n",
      "    val_targets = learning_targets[val_idx]\n",
      "    \n",
      "    # Extract temporal features for this fold\n",
      "    train_temp_features, val_temp_features, _, _ = lstm_generator.train_and_extract_features(\n",
      "        train_temporal, train_targets, val_temporal, val_targets\n",
      "    )\n",
      "    \n",
      "    # Extract spatial features (CapsNet/CNN)\n",
      "    train_spatial_features, val_spatial_features = capsnet_generator.train_and_extract_features(\n",
      "        train_images, train_targets, val_images, val_targets\n",
      "    )\n",
      "    \n",
      "    # Combine features\n",
      "    train_combined = np.concatenate([train_temp_features, train_spatial_features], axis=1)\n",
      "    val_combined = np.concatenate([val_temp_features, val_spatial_features], axis=1)\n",
      "    \n",
      "    # Train LightGBM meta-learner\n",
      "    lightgbm_model = LGBMRegressor(best_lightgbm_params)\n",
      "    lightgbm_model.fit(train_combined, train_targets)\n",
      "    \n",
      "    # Validate on this fold\n",
      "    val_predictions = lightgbm_model.predict(val_combined)\n",
      "    fold_rmse = np.sqrt(mean_squared_error(val_targets, val_predictions))\n",
      "    fold_scores.append(fold_rmse)\n",
      "    \n",
      "    print(f\"  Fold {fold + 1} RMSE: {fold_rmse:.4f}\")\n",
      "\n",
      "# Calculate cross-validation performance\n",
      "cv_mean = np.mean(fold_scores)\n",
      "cv_std = np.std(fold_scores)\n",
      "print(f\"5-Fold CV Results: {cv_mean:.4f} Â± {cv_std:.4f}\")\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
