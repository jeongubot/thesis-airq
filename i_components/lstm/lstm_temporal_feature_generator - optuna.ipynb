{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75aac7a0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from itertools import product\n",
    "from tqdm import tqdm \n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "118cc919",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class LSTMTemporalFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM component for hybrid models that produces 32-dimensional temporal features\n",
    "    Compatible with: CapsNet-LSTM-LightGBM, CNN-LSTM-LightGBM, CapsNet-LSTM\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=1, dropout=0.2, \n",
    "                 activation='relu', lstm_dropout=0.0):\n",
    "        super(LSTMTemporalFeatureExtractor, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers,\n",
    "            dropout=lstm_dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        else:\n",
    "            self.activation = nn.ReLU()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Feature extraction layer (32 dimensions for hybrid integration)\n",
    "        self.temporal_feature_layer = nn.Linear(hidden_size, 32)\n",
    "        \n",
    "        # Optional prediction layer (for CapsNet-LSTM model without LightGBM)\n",
    "        self.prediction_layer = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x, return_features_only=False):\n",
    "        \"\"\"\n",
    "        Forward pass with option to return only temporal features or full prediction\n",
    "        \n",
    "        Args:\n",
    "            x: Input sequences (batch_size, seq_len, features)\n",
    "            return_features_only: If True, returns only 32-dim temporal features\n",
    "                                If False, returns both features and prediction\n",
    "        \n",
    "        Returns:\n",
    "            temporal_features: 32-dimensional temporal features for hybrid models\n",
    "            prediction: PM2.5 prediction (only if return_features_only=False)\n",
    "        \"\"\"\n",
    "        # LSTM forward pass\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        last_output = lstm_out[:, -1, :]  # Use last timestep output\n",
    "        \n",
    "        # Extract 32-dimensional temporal features\n",
    "        temporal_features = self.activation(self.temporal_feature_layer(last_output))\n",
    "        temporal_features = self.dropout(temporal_features)\n",
    "        \n",
    "        if return_features_only:\n",
    "            return temporal_features\n",
    "        else:\n",
    "            # For CapsNet-LSTM model (without LightGBM)\n",
    "            prediction = self.prediction_layer(temporal_features)\n",
    "            return temporal_features, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54f1ce4c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class LSTMTemporalFeatureGenerator:\n",
    "    \"\"\"\n",
    "    LSTM component for temporal feature extraction in hybrid models\n",
    "    Designed to work within full pipeline cross-validation\n",
    "    \"\"\"\n",
    "    def __init__(self, best_params=None):\n",
    "        # Default hyperparameters (should be determined through separate tuning)\n",
    "        self.default_params = {\n",
    "            'hidden_size': 64,\n",
    "            'num_layers': 1,\n",
    "            'dropout': 0.2,\n",
    "            'activation': 'relu',\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 32,\n",
    "            'epochs': 50,\n",
    "            'timesteps': 60,  # Default temporal window size\n",
    "            'weight_decay': 0.0,\n",
    "            'grad_clip': 1.0,\n",
    "            'lstm_dropout': 0.0\n",
    "        }\n",
    "        \n",
    "        self.params = best_params if best_params else self.default_params\n",
    "        self.scaler = None\n",
    "        self.model = None\n",
    "        \n",
    "    def prepare_temporal_sequences(self, temporal_data, targets, timesteps=10):\n",
    "        \"\"\"\n",
    "        Prepare temporal sequences for a given fold\n",
    "        Called during each CV fold by the full pipeline\n",
    "        \"\"\"\n",
    "        # Scale temporal features\n",
    "        if self.scaler is None:\n",
    "            self.scaler = MinMaxScaler()\n",
    "            temporal_data_scaled = self.scaler.fit_transform(temporal_data)\n",
    "        else:\n",
    "            temporal_data_scaled = self.scaler.transform(temporal_data)\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = [], []\n",
    "        for i in range(len(temporal_data_scaled) - timesteps):\n",
    "            X.append(temporal_data_scaled[i:i+timesteps])\n",
    "            y.append(targets[i+timesteps])\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def train_and_extract_features(self, train_temporal_data, train_targets, val_temporal_data, val_targets, timesteps=None):\n",
    "        \"\"\"\n",
    "        Train LSTM and extract temporal features for current CV fold\n",
    "        This is called by the full pipeline during each fold\n",
    "        \n",
    "        IMPORTANT: val_temporal_data is validation data from the 80% learning set,\n",
    "                   NOT the 20% hold-out test set (to prevent data leakage)\n",
    "        \n",
    "        Returns:\n",
    "            train_features: 32D temporal features for training data\n",
    "            val_features: 32D temporal features for validation data (within learning set)\n",
    "        \"\"\"\n",
    "        # Use timesteps from params or default\n",
    "        if timesteps is None:\n",
    "            timesteps = self.params.get('timesteps', 60)\n",
    "            \n",
    "        # Prepare sequences for training\n",
    "        X_train, y_train = self.prepare_temporal_sequences(train_temporal_data, train_targets, timesteps)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "        \n",
    "        # Create data loader\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.params['batch_size'], shuffle=True)\n",
    "        \n",
    "        # Initialize LSTM model\n",
    "        self.model = LSTMTemporalFeatureExtractor(\n",
    "            input_size=train_temporal_data.shape[1],\n",
    "            hidden_size=self.params['hidden_size'],\n",
    "            num_layers=self.params['num_layers'],\n",
    "            dropout=self.params['dropout'],\n",
    "            activation=self.params['activation'],\n",
    "            lstm_dropout=self.params.get('lstm_dropout', 0.0)\n",
    "        )\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(\n",
    "            self.model.parameters(), \n",
    "            lr=self.params['learning_rate'],\n",
    "            weight_decay=self.params.get('weight_decay', 0.0)\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        self.model.train()\n",
    "        for epoch in range(self.params['epochs']):\n",
    "            total_loss = 0\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                _, predictions = self.model(batch_X, return_features_only=False)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(), \n",
    "                    max_norm=self.params.get('grad_clip', 1.0)\n",
    "                )\n",
    "                \n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        # Extract features from training data\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_features = self.model(X_train_tensor, return_features_only=True).numpy()\n",
    "        \n",
    "        # Prepare test sequences and extract features\n",
    "        X_val, _ = self.prepare_temporal_sequences(val_temporal_data, val_targets, timesteps)\n",
    "        X_val_tensor = torch.FloatTensor(X_val)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_features = self.model(X_val_tensor, return_features_only=True).numpy()\n",
    "        \n",
    "        # Debug: Verify numpy arrays are generated correctly\n",
    "        print(f\"  LSTM Debug - Train features shape: {train_features.shape}, type: {type(train_features)}\")\n",
    "        print(f\"  LSTM Debug - Val features shape: {val_features.shape}, type: {type(val_features)}\")\n",
    "        print(f\"  LSTM Debug - Feature sample: {train_features[0][:5]}...\")  # First 5 values\n",
    "        \n",
    "        return train_features, val_features, y_train, X_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2784e3eb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TemporalDataLoader:\n",
    "    \"\"\"\n",
    "    Data loader for temporal features across all days\n",
    "    Supports full pipeline cross-validation\n",
    "    \"\"\"\n",
    "    def __init__(self, days=['7_24', '10_19', '11_10']):\n",
    "        self.days = days\n",
    "        self.temporal_features = ['pm10', 'temperature', 'humidity']\n",
    "    \n",
    "    def load_temporal_data(self, day):\n",
    "        \"\"\"\n",
    "        Load temporal data for a specific day\n",
    "        Returns raw temporal data and targets for CV splitting\n",
    "        \"\"\"\n",
    "        matched_file = f'../../dataset/c_matched_spatio_temporal_data/matched_{day}.csv'\n",
    "        if not os.path.exists(matched_file):\n",
    "            raise FileNotFoundError(f\"Matched data not found: {matched_file}\")\n",
    "        \n",
    "        df = pd.read_csv(matched_file)\n",
    "        \n",
    "        # Sort by timestamp for proper time series handling\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "        # Get available temporal features\n",
    "        available_features = [col for col in self.temporal_features if col in df.columns]\n",
    "        \n",
    "        if not available_features:\n",
    "            raise ValueError(f\"No temporal features found in {day}\")\n",
    "        \n",
    "        # Remove rows with missing values\n",
    "        required_cols = available_features + ['pm2.5']\n",
    "        df_clean = df.dropna(subset=required_cols)\n",
    "        \n",
    "        temporal_data = df_clean[available_features].values\n",
    "        targets = df_clean['pm2.5'].values\n",
    "        \n",
    "        return temporal_data, targets, available_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7c6f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning_lstm(day, n_trials=20):\n",
    "    \"\"\"\n",
    "    Hyperparameter tuning for LSTM component using Optuna.\n",
    "    \"\"\"\n",
    "    print(f\"LSTM Hyperparameter Tuning for {day}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Load data\n",
    "    data_loader = TemporalDataLoader()\n",
    "    temporal_data, targets, feature_names = data_loader.load_temporal_data(day)\n",
    "\n",
    "    n_total = len(temporal_data)\n",
    "    n_learning = int(n_total * 0.8)\n",
    "    learning_temporal = temporal_data[:n_learning]\n",
    "    learning_targets = targets[:n_learning]\n",
    "\n",
    "    n_tune_train = int(n_learning * 0.8)\n",
    "    tune_train_temporal = learning_temporal[:n_tune_train]\n",
    "    tune_train_targets = learning_targets[:n_tune_train]\n",
    "    tune_val_temporal = learning_temporal[n_tune_train:]\n",
    "    tune_val_targets = learning_targets[n_tune_train:]\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'hidden_size': trial.suggest_categorical('hidden_size', [32, 64, 128, 256]),\n",
    "            'num_layers': trial.suggest_int('num_layers', 1, 3),\n",
    "            'dropout': trial.suggest_categorical('dropout', [0.1, 0.2, 0.3, 0.4]),\n",
    "            'activation': trial.suggest_categorical('activation', ['relu', 'tanh', 'gelu']),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.01, log=True),\n",
    "            'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64]),\n",
    "            'epochs': trial.suggest_categorical('epochs', [10, 30, 50]),\n",
    "            'timesteps': trial.suggest_categorical('timesteps', [10, 30, 40, 60]),\n",
    "            'weight_decay': trial.suggest_categorical('weight_decay', [0.0, 1e-5, 1e-4]),\n",
    "            'grad_clip': trial.suggest_categorical('grad_clip', [0.5, 1.0, 2.0]),\n",
    "            'lstm_dropout': trial.suggest_categorical('lstm_dropout', [0.0, 0.1, 0.2])\n",
    "        }\n",
    "        lstm_gen = LSTMTemporalFeatureGenerator(params)\n",
    "        try:\n",
    "            train_features, val_features, train_y, val_len = lstm_gen.train_and_extract_features(\n",
    "                tune_train_temporal, tune_train_targets, tune_val_temporal, tune_val_targets,\n",
    "                timesteps=params['timesteps']\n",
    "            )\n",
    "            val_y = tune_val_targets[params['timesteps']:]\n",
    "            if len(val_y) == len(val_features):\n",
    "                from sklearn.linear_model import LinearRegression\n",
    "                lr = LinearRegression()\n",
    "                lr.fit(train_features, train_y)\n",
    "                val_pred = lr.predict(val_features)\n",
    "                mse = mean_squared_error(val_y, val_pred)\n",
    "                return mse\n",
    "            else:\n",
    "                return float('inf')\n",
    "        except Exception as e:\n",
    "            print(f\"Optuna trial error: {e}\")\n",
    "            return float('inf')\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_score = study.best_value\n",
    "\n",
    "    os.makedirs('models/lstm_temporal', exist_ok=True)\n",
    "    with open(f'models/lstm_temporal/{day}_best_params.json', 'w') as f:\n",
    "        json.dump(best_params, f, indent=2)\n",
    "\n",
    "    print(f\"Best parameters for {day}: {best_params}\")\n",
    "    print(f\"Best validation RMSE: {np.sqrt(best_score):.4f}\")\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddf4cb15",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution for LSTM temporal feature extraction\n",
    "    Focused on component preparation for full pipeline CV\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"LSTM TEMPORAL FEATURE EXTRACTION - PIPELINE READY\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Purpose: Prepare LSTM component for full pipeline cross-validation\")\n",
    "    print(\"- Hyperparameter tuning for each day\")\n",
    "    print(\"- Component ready for CV integration\")\n",
    "    print(\"- No standalone evaluation (done in full pipeline)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    days = ['7_24', '10_19', '11_10']\n",
    "    \n",
    "    # Step 1: Hyperparameter tuning for each day\n",
    "    print(\"\\nStep 1: LSTM Hyperparameter Tuning\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for day in days:\n",
    "        try:\n",
    "            print(f\"\\nTuning LSTM parameters for {day}...\")\n",
    "            best_params = hyperparameter_tuning_lstm(day, n_trials=20)\n",
    "            print(f\" {day} tuning complete\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Error tuning {day}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"LSTM COMPONENT PREPARATION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Best parameters saved for each day\")\n",
    "    print(\"LSTMTemporalFeatureGenerator class ready\")\n",
    "    print(\"TemporalDataLoader class ready\")\n",
    "    print(\"\\nNext Steps:\")\n",
    "    print(\"1. Integration team: Use LSTMTemporalFeatureGenerator in full pipeline\")\n",
    "    print(\"2. Full pipeline CV: Wrap entire models in 5-fold cross-validation\")\n",
    "    print(\"3. Models to evaluate: CapsNet-LSTM-LightGBM, CNN-LSTM-LightGBM, CapsNet-LSTM\")\n",
    "    print(\"4. Statistical analysis: Compare 5-fold results across models\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Example usage for integration team\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CORRECT 5-FOLD CV INTEGRATION:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\"\"\n",
    "# PHASE 1: Hyperparameter Tuning (run once per day)\n",
    "from lstm_temporal_feature_generator import hyperparameter_tuning_lstm\n",
    "best_params = hyperparameter_tuning_lstm('7_24', max_combinations=10)\n",
    "\n",
    "# PHASE 2: 5-Fold Cross-Validation (main pipeline)\n",
    "from lstm_temporal_feature_generator import LSTMTemporalFeatureGenerator, TemporalDataLoader\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Initialize components\n",
    "data_loader = TemporalDataLoader()\n",
    "lstm_generator = LSTMTemporalFeatureGenerator(best_params)\n",
    "\n",
    "# Load data and split into learning (80%) and hold-out (20%)\n",
    "temporal_data, targets, _ = data_loader.load_temporal_data('7_24')\n",
    "n_total = len(temporal_data)\n",
    "n_learning = int(n_total * 0.8)\n",
    "\n",
    "learning_temporal = temporal_data[:n_learning]  # 80% learning set\n",
    "learning_targets = targets[:n_learning]\n",
    "holdout_temporal = temporal_data[n_learning:]   # 20% hold-out (untouched)\n",
    "holdout_targets = targets[n_learning:]\n",
    "\n",
    "# 5-Fold TimeSeriesSplit on 80% learning set\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "fold_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(learning_temporal)):\n",
    "    print(f\"Fold {fold + 1}/5\")\n",
    "    \n",
    "    # Split data for this fold (expanding window)\n",
    "    train_temporal = learning_temporal[train_idx]  # Growing train set\n",
    "    val_temporal = learning_temporal[val_idx]      # Fixed-size val set\n",
    "    train_targets = learning_targets[train_idx]\n",
    "    val_targets = learning_targets[val_idx]\n",
    "    \n",
    "    # Extract temporal features for this fold\n",
    "    train_temp_features, val_temp_features, _, _ = lstm_generator.train_and_extract_features(\n",
    "        train_temporal, train_targets, val_temporal, val_targets\n",
    "    )\n",
    "    \n",
    "    # Extract spatial features (CapsNet/CNN)\n",
    "    train_spatial_features, val_spatial_features = capsnet_generator.train_and_extract_features(\n",
    "        train_images, train_targets, val_images, val_targets\n",
    "    )\n",
    "    \n",
    "    # Combine features\n",
    "    train_combined = np.concatenate([train_temp_features, train_spatial_features], axis=1)\n",
    "    val_combined = np.concatenate([val_temp_features, val_spatial_features], axis=1)\n",
    "    \n",
    "    # Train LightGBM meta-learner\n",
    "    lightgbm_model = LGBMRegressor(best_lightgbm_params)\n",
    "    lightgbm_model.fit(train_combined, train_targets)\n",
    "    \n",
    "    # Validate on this fold\n",
    "    val_predictions = lightgbm_model.predict(val_combined)\n",
    "    fold_rmse = np.sqrt(mean_squared_error(val_targets, val_predictions))\n",
    "    fold_scores.append(fold_rmse)\n",
    "    \n",
    "    print(f\"  Fold {fold + 1} RMSE: {fold_rmse:.4f}\")\n",
    "\n",
    "# Calculate cross-validation performance\n",
    "cv_mean = np.mean(fold_scores)\n",
    "cv_std = np.std(fold_scores)\n",
    "print(f\"5-Fold CV Results: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "\"\"\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e84b21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-21 16:35:21,224] A new study created in memory with name: no-name-1c419e31-ede5-47a4-a8e9-b3eb88611430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LSTM TEMPORAL FEATURE EXTRACTION - PIPELINE READY\n",
      "================================================================================\n",
      "Purpose: Prepare LSTM component for full pipeline cross-validation\n",
      "- Hyperparameter tuning for each day\n",
      "- Component ready for CV integration\n",
      "- No standalone evaluation (done in full pipeline)\n",
      "================================================================================\n",
      "\n",
      "Step 1: LSTM Hyperparameter Tuning\n",
      "--------------------------------------------------\n",
      "\n",
      "Tuning LSTM parameters for 7_24...\n",
      "LSTM Hyperparameter Tuning for 7_24\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b385202e85549d6b20d51c237eb93f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-07-21 16:35:30,830] Trial 0 failed with parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.4, 'activation': 'tanh', 'learning_rate': 0.00044791204007958626, 'batch_size': 32, 'epochs': 10, 'timesteps': 60, 'weight_decay': 0.0001, 'grad_clip': 2.0, 'lstm_dropout': 0.0} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Micah\\anaconda3\\envs\\tfenv\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_25100\\1028388552.py\", line 39, in objective\n",
      "    train_features, val_features, train_y, val_len = lstm_gen.train_and_extract_features(\n",
      "  File \"C:\\Users\\Micah\\AppData\\Local\\Temp\\ipykernel_25100\\1809810937.py\", line 98, in train_and_extract_features\n",
      "    loss.backward()\n",
      "  File \"c:\\Users\\Micah\\anaconda3\\envs\\tfenv\\lib\\site-packages\\torch\\_tensor.py\", line 648, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"c:\\Users\\Micah\\anaconda3\\envs\\tfenv\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 353, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"c:\\Users\\Micah\\anaconda3\\envs\\tfenv\\lib\\site-packages\\torch\\autograd\\graph.py\", line 824, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "[W 2025-07-21 16:35:30,833] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 24\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTuning LSTM parameters for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mday\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m     best_params \u001b[38;5;241m=\u001b[39m \u001b[43mhyperparameter_tuning_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mday\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mday\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tuning complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[19], line 58\u001b[0m, in \u001b[0;36mhyperparameter_tuning_lstm\u001b[1;34m(day, n_trials)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     57\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[0;32m     61\u001b[0m best_score \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_value\n",
      "File \u001b[1;32mc:\\Users\\Micah\\anaconda3\\envs\\tfenv\\lib\\site-packages\\optuna\\study\\study.py:489\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    389\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    397\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    398\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Micah\\anaconda3\\envs\\tfenv\\lib\\site-packages\\optuna\\study\\_optimize.py:64\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 64\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Micah\\anaconda3\\envs\\tfenv\\lib\\site-packages\\optuna\\study\\_optimize.py:161\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\Micah\\anaconda3\\envs\\tfenv\\lib\\site-packages\\optuna\\study\\_optimize.py:253\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    249\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    252\u001b[0m ):\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\Micah\\anaconda3\\envs\\tfenv\\lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[19], line 39\u001b[0m, in \u001b[0;36mhyperparameter_tuning_lstm.<locals>.objective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     37\u001b[0m lstm_gen \u001b[38;5;241m=\u001b[39m LSTMTemporalFeatureGenerator(params)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     train_features, val_features, train_y, val_len \u001b[38;5;241m=\u001b[39m \u001b[43mlstm_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_extract_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtune_train_temporal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune_train_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune_val_temporal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune_val_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimesteps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     val_y \u001b[38;5;241m=\u001b[39m tune_val_targets[params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimesteps\u001b[39m\u001b[38;5;124m'\u001b[39m]:]\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val_y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_features):\n",
      "Cell \u001b[1;32mIn[17], line 98\u001b[0m, in \u001b[0;36mLSTMTemporalFeatureGenerator.train_and_extract_features\u001b[1;34m(self, train_temporal_data, train_targets, val_temporal_data, val_targets, timesteps)\u001b[0m\n\u001b[0;32m     96\u001b[0m _, predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch_X, return_features_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     97\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, batch_y)\n\u001b[1;32m---> 98\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[0;32m    101\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \n\u001b[0;32m    103\u001b[0m     max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad_clip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    104\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Micah\\anaconda3\\envs\\tfenv\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Micah\\anaconda3\\envs\\tfenv\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Micah\\anaconda3\\envs\\tfenv\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
