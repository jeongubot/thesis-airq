{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c9a0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import json\n",
    "from itertools import product\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6408f77f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "DAY_PREFIXES = ['11_10_', '7_24_', '10_19_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cdfd5ca8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class EnhancedLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=1, dropout=0.2, \n",
    "                 activation='relu', output_size=1):\n",
    "        super(EnhancedLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers with configurable parameters\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,  # Dropout only for multi-layer\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Configurable activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        else:\n",
    "            self.activation = nn.ReLU()  # Default\n",
    "        \n",
    "        # Fully connected layers with dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, 32)\n",
    "        self.fc2 = nn.Linear(32, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM forward pass\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "        # Use the last output\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layers with activation and dropout\n",
    "        out = self.activation(self.fc1(last_output))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52f09882",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class LSTMHyperparameterTuner:\n",
    "    def __init__(self):\n",
    "        # Define hyperparameter search space\n",
    "        self.hyperparameter_space = {\n",
    "            'hidden_size': [32, 64, 128, 256],\n",
    "            'num_layers': [1, 2, 3],\n",
    "            'dropout': [0.1, 0.2, 0.3, 0.5],\n",
    "            'activation': ['relu', 'tanh', 'leaky_relu', 'gelu'],\n",
    "            'learning_rate': [0.0001, 0.001, 0.01, 0.1],\n",
    "            'batch_size': [8, 16, 32, 64],\n",
    "            'epochs': [2, 3]\n",
    "        }\n",
    "        \n",
    "        self.best_params = {}\n",
    "        self.best_score = float('inf')\n",
    "        self.tuning_results = []\n",
    "        \n",
    "    def create_hyperparameter_combinations(self, max_combinations=50):\n",
    "\n",
    "        # Get all possible combinations\n",
    "        keys = list(self.hyperparameter_space.keys())\n",
    "        values = list(self.hyperparameter_space.values())\n",
    "        all_combinations = list(product(*values))\n",
    "        \n",
    "        # If too many combinations, sample intelligently\n",
    "        if len(all_combinations) > max_combinations:\n",
    "            # Prioritize certain combinations\n",
    "            np.random.seed(42)  # For reproducibility\n",
    "            selected_indices = np.random.choice(\n",
    "                len(all_combinations), \n",
    "                size=max_combinations, \n",
    "                replace=False\n",
    "            )\n",
    "            combinations = [all_combinations[i] for i in selected_indices]\n",
    "        else:\n",
    "            combinations = all_combinations\n",
    "        \n",
    "        # Convert to list of dictionaries\n",
    "        param_combinations = []\n",
    "        for combo in combinations:\n",
    "            param_dict = dict(zip(keys, combo))\n",
    "            param_combinations.append(param_dict)\n",
    "        \n",
    "        return param_combinations\n",
    "    \n",
    "    def validate_hyperparameters(self, params, X_train, y_train, X_val, y_val, input_size):\n",
    " \n",
    "        try:\n",
    "            # Convert to PyTorch tensors\n",
    "            X_train_tensor = torch.FloatTensor(X_train)\n",
    "            y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "            X_val_tensor = torch.FloatTensor(X_val)\n",
    "            y_val_tensor = torch.FloatTensor(y_val).unsqueeze(1)\n",
    "            \n",
    "            # Create data loaders\n",
    "            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "            val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "            \n",
    "            train_loader = DataLoader(\n",
    "                train_dataset, \n",
    "                batch_size=params['batch_size'], \n",
    "                shuffle=True\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset, \n",
    "                batch_size=params['batch_size'], \n",
    "                shuffle=False\n",
    "            )\n",
    "            \n",
    "            # Initialize model with hyperparameters\n",
    "            model = EnhancedLSTM(\n",
    "                input_size=input_size,\n",
    "                hidden_size=params['hidden_size'],\n",
    "                num_layers=params['num_layers'],\n",
    "                dropout=params['dropout'],\n",
    "                activation=params['activation']\n",
    "            )\n",
    "            \n",
    "            # Loss and optimizer\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "            \n",
    "            # Training loop with early stopping\n",
    "            best_val_loss = float('inf')\n",
    "            total_epochs = params['epochs']\n",
    "            patience = int(0.2 * total_epochs)\n",
    "            patience_counter = 0\n",
    "            \n",
    "            for epoch in tqdm(range(params['epochs']), desc=\"Epochs\"):\n",
    "                # Training\n",
    "                model.train()\n",
    "                train_loss = 0\n",
    "                for batch_X, batch_y in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Gradient clipping to prevent exploding gradients\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    train_loss += loss.item()\n",
    "                \n",
    "                # Validation\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                with torch.no_grad():\n",
    "                    for batch_X, batch_y in val_loader:\n",
    "                        outputs = model(batch_X)\n",
    "                        loss = criterion(outputs, batch_y)\n",
    "                        val_loss += loss.item()\n",
    "                \n",
    "                train_loss /= len(train_loader)\n",
    "                val_loss /= len(val_loader)\n",
    "                \n",
    "                # Early stopping check\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        break\n",
    "            \n",
    "            # Calculate additional metrics\n",
    "            model.eval()\n",
    "            val_predictions = []\n",
    "            val_targets = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_X, batch_y in val_loader:\n",
    "                    outputs = model(batch_X)\n",
    "                    val_predictions.extend(outputs.squeeze().numpy())\n",
    "                    val_targets.extend(batch_y.squeeze().numpy())\n",
    "            \n",
    "            val_predictions = np.array(val_predictions)\n",
    "            val_targets = np.array(val_targets)\n",
    "            \n",
    "            # Calculate comprehensive metrics\n",
    "            mse = mean_squared_error(val_targets, val_predictions)\n",
    "            mae = mean_absolute_error(val_targets, val_predictions)\n",
    "            r2 = r2_score(val_targets, val_predictions)\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            n = len(val_targets)\n",
    "            p = val_predictions.shape[0] if val_predictions.ndim > 1 else 1\n",
    "            adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "            \n",
    "            \n",
    "            # Convert all numpy.float32 to standard Python float for JSON serialization\n",
    "            return {\n",
    "                'val_loss': float(best_val_loss),\n",
    "                'mse': float(mse),\n",
    "                'mae': float(mae),\n",
    "                'rmse': float(rmse),\n",
    "                'r2': float(r2),\n",
    "                'adj_r2': float(adj_r2),\n",
    "                'model': model,\n",
    "                'epochs_trained': int(epoch + 1) # Ensure integer\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with hyperparameters {params}: {str(e)}\")\n",
    "            return {\n",
    "                'val_loss': float('inf'),\n",
    "                'mse': float('inf'),\n",
    "                'mae': float('inf'),\n",
    "                'rmse': float('inf'),\n",
    "                'r2': -float('inf'),\n",
    "                'model': None,\n",
    "                'epochs_trained': 0\n",
    "            }\n",
    "    \n",
    "    def tune_hyperparameters(self, X_train, y_train, X_val, y_val, input_size, \n",
    "                           day_prefix, max_combinations=3):\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"HYPERPARAMETER TUNING FOR {day_prefix}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Framework: Train/Val split only (Test set untouched)\")\n",
    "        print(f\"Search space: {max_combinations} combinations\")\n",
    "        print(f\"Validation strategy: Time series split compliance\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Generate hyperparameter combinations\n",
    "        param_combinations = self.create_hyperparameter_combinations(max_combinations)\n",
    "        \n",
    "        print(f\"Testing {len(param_combinations)} hyperparameter combinations...\")\n",
    "        \n",
    "        best_model = None\n",
    "        best_params = None\n",
    "        best_score = float('inf')\n",
    "        \n",
    "        for i, params in enumerate(tqdm(param_combinations, desc=\"Testing hyperparameters\")):\n",
    "            print(f\"\\nCombination {i+1}/{len(param_combinations)}\")\n",
    "            print(f\"Params: {params}\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Validate hyperparameters (ONLY using train/val data)\n",
    "            results = self.validate_hyperparameters(\n",
    "                params, X_train, y_train, X_val, y_val, input_size\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Store results\n",
    "            result_entry = {\n",
    "                'combination': i + 1,\n",
    "                'params': params,\n",
    "                'val_loss': results['val_loss'],\n",
    "                'mse': results['mse'],\n",
    "                'mae': results['mae'],\n",
    "                'rmse': results['rmse'],\n",
    "                'r2': results['r2'],\n",
    "                'adj_r2': results['adj_r2'],\n",
    "                'epochs_trained': results['epochs_trained'],\n",
    "                'training_time': end_time - start_time\n",
    "            }\n",
    "            \n",
    "            self.tuning_results.append(result_entry)\n",
    "            \n",
    "            print(f\"Val Loss: {results['val_loss']:.4f}, RMSE: {results['rmse']:.4f}, \"\n",
    "            f\"MAE: {results['mae']:.4f}, R2: {results['r2']:.4f}, Adj R2: {results['adj_r2']:.4f}, \"\n",
    "            f\"Time: {end_time - start_time:.2f}s\")\n",
    "            \n",
    "            # Update best model\n",
    "            if results['val_loss'] < best_score:\n",
    "                best_score = results['val_loss']\n",
    "                best_params = params.copy()\n",
    "                best_model = results['model']\n",
    "                print(f\"[BEST] NEW BEST MODEL! Val Loss: {best_score:.4f}\")\n",
    "        \n",
    "        # Save best hyperparameters and results\n",
    "        self.best_params[day_prefix] = best_params\n",
    "        self.best_score = best_score\n",
    "        \n",
    "        # Save tuning results\n",
    "        results_path = f'{day_prefix}hyperparameter_tuning_results.json'\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(self.tuning_results, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"HYPERPARAMETER TUNING COMPLETE FOR {day_prefix}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Best hyperparameters:\")\n",
    "        for key, value in best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        print(f\"Best validation loss: {best_score:.4f}\")\n",
    "        print(f\"Results saved to: {results_path}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return best_model, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "107cf181",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_optimized_lstm(X_train, y_train, X_val, y_val, input_size, \n",
    "                        day_prefix, tune_hyperparameters=True):\n",
    "\n",
    "    if tune_hyperparameters:\n",
    "        # Perform hyperparameter tuning\n",
    "        tuner = LSTMHyperparameterTuner()\n",
    "        best_model, best_params = tuner.tune_hyperparameters(\n",
    "            X_train, y_train, X_val, y_val, input_size, day_prefix\n",
    "        )\n",
    "        \n",
    "        # Save the best model\n",
    "        model_path = f'{day_prefix}lstm_model_optimized.pth'\n",
    "        torch.save(best_model.state_dict(), model_path)\n",
    "        \n",
    "        # Save hyperparameters\n",
    "        params_path = f'{day_prefix}best_hyperparameters.json'\n",
    "        with open(params_path, 'w') as f:\n",
    "            json.dump(best_params, f, indent=2)\n",
    "        \n",
    "        print(f\"Optimized model saved: {model_path}\")\n",
    "        print(f\"Best hyperparameters saved: {params_path}\")\n",
    "        \n",
    "        return best_model, best_params\n",
    "    \n",
    "    else:\n",
    "        # Use default hyperparameters (original model)\n",
    "        print(f\"Training with default hyperparameters for {day_prefix}\")\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "        X_val_tensor = torch.FloatTensor(X_val)\n",
    "        y_val_tensor = torch.FloatTensor(y_val).unsqueeze(1)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "        \n",
    "        # Initialize model with default parameters\n",
    "        model = EnhancedLSTM(input_size=input_size)\n",
    "        \n",
    "        # Loss and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Training loop\n",
    "        best_val_loss = float('inf')\n",
    "        patience = 10\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(100):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_X, batch_y in val_loader:\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            val_loss /= len(val_loader)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                model_path = f'{day_prefix}lstm_model_default.pth'\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "        \n",
    "        return model, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a59ef55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENHANCED LSTM TRAINING WITH HYPERPARAMETER TUNING\n",
      "================================================================================\n",
      "Framework Compliance:\n",
      "[OK] Test set remains untouched during training and validation\n",
      "[OK] Hyperparameter tuning uses only train/val splits\n",
      "[OK] Time series split methodology maintained\n",
      "[OK] No data leakage prevention implemented\n",
      "================================================================================\n",
      "\n",
      "Processing 11_10_:\n",
      "Train shape: (83346, 60, 3), Val shape: (11851, 60, 3)\n",
      "Input features: 3\n",
      "Framework compliance: Test data not accessed\n",
      "\n",
      "============================================================\n",
      "HYPERPARAMETER TUNING FOR 11_10_\n",
      "============================================================\n",
      "Framework: Train/Val split only (Test set untouched)\n",
      "Search space: 3 combinations\n",
      "Validation strategy: Time series split compliance\n",
      "============================================================\n",
      "Testing 3 hyperparameter combinations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c885ba674c3043b999baef8f6be56783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing hyperparameters:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination 1/3\n",
      "Params: {'hidden_size': 128, 'num_layers': 3, 'dropout': 0.5, 'activation': 'relu', 'learning_rate': 0.001, 'batch_size': 8, 'epochs': 3}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d1719c555844a28134202ae589583a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 134.6281, RMSE: 11.6190, MAE: 9.8357, R2: -0.0096, Adj R2: -0.0097, Time: 325.01s\n",
      "[BEST] NEW BEST MODEL! Val Loss: 134.6281\n",
      "\n",
      "Combination 2/3\n",
      "Params: {'hidden_size': 128, 'num_layers': 3, 'dropout': 0.1, 'activation': 'tanh', 'learning_rate': 0.1, 'batch_size': 64, 'epochs': 3}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf9293489da466bbe72ae8088d76118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 138.5528, RMSE: 11.8848, MAE: 10.2239, R2: -0.0563, Adj R2: -0.0564, Time: 298.63s\n",
      "\n",
      "Combination 3/3\n",
      "Params: {'hidden_size': 64, 'num_layers': 3, 'dropout': 0.2, 'activation': 'leaky_relu', 'learning_rate': 0.1, 'batch_size': 32, 'epochs': 2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c8a861a6674783b10b8b0aac4739bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 135.4642, RMSE: 11.6304, MAE: 9.9744, R2: -0.0116, Adj R2: -0.0117, Time: 108.90s\n",
      "\n",
      "============================================================\n",
      "HYPERPARAMETER TUNING COMPLETE FOR 11_10_\n",
      "============================================================\n",
      "Best hyperparameters:\n",
      "  hidden_size: 128\n",
      "  num_layers: 3\n",
      "  dropout: 0.5\n",
      "  activation: relu\n",
      "  learning_rate: 0.001\n",
      "  batch_size: 8\n",
      "  epochs: 3\n",
      "Best validation loss: 134.6281\n",
      "Results saved to: 11_10_hyperparameter_tuning_results.json\n",
      "============================================================\n",
      "Optimized model saved: 11_10_lstm_model_optimized.pth\n",
      "Best hyperparameters saved: 11_10_best_hyperparameters.json\n",
      "Model for 11_10_ trained with optimized hyperparameters\n",
      "\n",
      "Processing 7_24_:\n",
      "Train shape: (45174, 60, 1), Val shape: (6402, 60, 1)\n",
      "Input features: 1\n",
      "Framework compliance: Test data not accessed\n",
      "\n",
      "============================================================\n",
      "HYPERPARAMETER TUNING FOR 7_24_\n",
      "============================================================\n",
      "Framework: Train/Val split only (Test set untouched)\n",
      "Search space: 3 combinations\n",
      "Validation strategy: Time series split compliance\n",
      "============================================================\n",
      "Testing 3 hyperparameter combinations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39bcf7560c054ba8acd1060aabe5cb8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing hyperparameters:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination 1/3\n",
      "Params: {'hidden_size': 128, 'num_layers': 3, 'dropout': 0.5, 'activation': 'relu', 'learning_rate': 0.001, 'batch_size': 8, 'epochs': 3}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863ed8ff2e59484ba72548892bed5001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 40.6437, RMSE: 6.3770, MAE: 3.6727, R2: -0.1396, Adj R2: -0.1397, Time: 292.90s\n",
      "[BEST] NEW BEST MODEL! Val Loss: 40.6437\n",
      "\n",
      "Combination 2/3\n",
      "Params: {'hidden_size': 128, 'num_layers': 3, 'dropout': 0.1, 'activation': 'tanh', 'learning_rate': 0.1, 'batch_size': 64, 'epochs': 3}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72393c7fcc314013988bea7d94d2314a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 37.2871, RMSE: 7.2644, MAE: 4.5466, R2: -0.4788, Adj R2: -0.4790, Time: 107.16s\n",
      "[BEST] NEW BEST MODEL! Val Loss: 37.2871\n",
      "\n",
      "Combination 3/3\n",
      "Params: {'hidden_size': 64, 'num_layers': 3, 'dropout': 0.2, 'activation': 'leaky_relu', 'learning_rate': 0.1, 'batch_size': 32, 'epochs': 2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3f4db9d740491799a43bc2beef41d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 39.8337, RMSE: 6.3195, MAE: 3.6576, R2: -0.1191, Adj R2: -0.1193, Time: 57.90s\n",
      "\n",
      "============================================================\n",
      "HYPERPARAMETER TUNING COMPLETE FOR 7_24_\n",
      "============================================================\n",
      "Best hyperparameters:\n",
      "  hidden_size: 128\n",
      "  num_layers: 3\n",
      "  dropout: 0.1\n",
      "  activation: tanh\n",
      "  learning_rate: 0.1\n",
      "  batch_size: 64\n",
      "  epochs: 3\n",
      "Best validation loss: 37.2871\n",
      "Results saved to: 7_24_hyperparameter_tuning_results.json\n",
      "============================================================\n",
      "Optimized model saved: 7_24_lstm_model_optimized.pth\n",
      "Best hyperparameters saved: 7_24_best_hyperparameters.json\n",
      "Model for 7_24_ trained with optimized hyperparameters\n",
      "\n",
      "Processing 10_19_:\n",
      "Train shape: (86641, 60, 3), Val shape: (12325, 60, 3)\n",
      "Input features: 3\n",
      "Framework compliance: Test data not accessed\n",
      "\n",
      "============================================================\n",
      "HYPERPARAMETER TUNING FOR 10_19_\n",
      "============================================================\n",
      "Framework: Train/Val split only (Test set untouched)\n",
      "Search space: 3 combinations\n",
      "Validation strategy: Time series split compliance\n",
      "============================================================\n",
      "Testing 3 hyperparameter combinations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757ee4dfe6c843379a612b5223d92245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing hyperparameters:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination 1/3\n",
      "Params: {'hidden_size': 128, 'num_layers': 3, 'dropout': 0.5, 'activation': 'relu', 'learning_rate': 0.001, 'batch_size': 8, 'epochs': 3}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d26d9851ff42859f6f2e692ab9ac19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 46.1481, RMSE: 6.7937, MAE: 4.5546, R2: -0.0902, Adj R2: -0.0902, Time: 531.09s\n",
      "[BEST] NEW BEST MODEL! Val Loss: 46.1481\n",
      "\n",
      "Combination 2/3\n",
      "Params: {'hidden_size': 128, 'num_layers': 3, 'dropout': 0.1, 'activation': 'tanh', 'learning_rate': 0.1, 'batch_size': 64, 'epochs': 3}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e722d18610034f9dba7940494d7a6d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 42.4871, RMSE: 6.5370, MAE: 4.4922, R2: -0.0093, Adj R2: -0.0094, Time: 204.19s\n",
      "[BEST] NEW BEST MODEL! Val Loss: 42.4871\n",
      "\n",
      "Combination 3/3\n",
      "Params: {'hidden_size': 64, 'num_layers': 3, 'dropout': 0.2, 'activation': 'leaky_relu', 'learning_rate': 0.1, 'batch_size': 32, 'epochs': 2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4387e7530d04051be5d19a692df0b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 42.3982, RMSE: 6.5364, MAE: 4.3857, R2: -0.0091, Adj R2: -0.0092, Time: 106.71s\n",
      "[BEST] NEW BEST MODEL! Val Loss: 42.3982\n",
      "\n",
      "============================================================\n",
      "HYPERPARAMETER TUNING COMPLETE FOR 10_19_\n",
      "============================================================\n",
      "Best hyperparameters:\n",
      "  hidden_size: 64\n",
      "  num_layers: 3\n",
      "  dropout: 0.2\n",
      "  activation: leaky_relu\n",
      "  learning_rate: 0.1\n",
      "  batch_size: 32\n",
      "  epochs: 2\n",
      "Best validation loss: 42.3982\n",
      "Results saved to: 10_19_hyperparameter_tuning_results.json\n",
      "============================================================\n",
      "Optimized model saved: 10_19_lstm_model_optimized.pth\n",
      "Best hyperparameters saved: 10_19_best_hyperparameters.json\n",
      "Model for 10_19_ trained with optimized hyperparameters\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*80)\n",
    "    print(\"ENHANCED LSTM TRAINING WITH HYPERPARAMETER TUNING\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Framework Compliance:\")\n",
    "    print(\"[OK] Test set remains untouched during training and validation\")\n",
    "    print(\"[OK] Hyperparameter tuning uses only train/val splits\")\n",
    "    print(\"[OK] Time series split methodology maintained\")\n",
    "    print(\"[OK] No data leakage prevention implemented\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Configuration\n",
    "    TUNE_HYPERPARAMETERS = True  # Set to False for default training\n",
    "    \n",
    "    for prefix in DAY_PREFIXES:\n",
    "        npz_path = f'../../{prefix}lstm_preprocessed_data.npz'\n",
    "        if not os.path.exists(npz_path):\n",
    "            print(f\"File not found: {npz_path}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Load preprocessed data\n",
    "        data = np.load(npz_path, allow_pickle=True)\n",
    "        X_train, y_train = data['X_train'], data['y_train']\n",
    "        X_val, y_val = data['X_val'], data['y_val']\n",
    "        # NOTE: X_test and y_test are NOT used during training/tuning\n",
    "        \n",
    "        input_size = X_train.shape[2]  # Number of features\n",
    "\n",
    "        print(f\"\\nProcessing {prefix}:\")\n",
    "        print(f\"Train shape: {X_train.shape}, Val shape: {X_val.shape}\")\n",
    "        print(f\"Input features: {input_size}\")\n",
    "        print(f\"Framework compliance: Test data not accessed\")\n",
    "\n",
    "        # Train with hyperparameter tuning or default parameters\n",
    "        model, best_params = train_optimized_lstm(\n",
    "            X_train, y_train, X_val, y_val, input_size, \n",
    "            prefix, tune_hyperparameters=TUNE_HYPERPARAMETERS\n",
    "        )\n",
    "\n",
    "        if best_params:\n",
    "            print(f\"Model for {prefix} trained with optimized hyperparameters\")\n",
    "        else:\n",
    "            print(f\"Model for {prefix} trained with default hyperparameters\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
