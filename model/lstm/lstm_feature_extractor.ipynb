{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "223a2fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import json # Import json to load hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aea9a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your new EnhancedLSTM model class\n",
    "from lstm_model import EnhancedLSTM # Assuming the file is named lstm_model_enhanced_fixed.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a68eaae",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "DAY_PREFIXES = ['11_10_', '7_24_', '10_19_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74e27a52",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_lstm_features_pytorch(model_path, X_test, input_size, hyperparams):\n",
    "    \"\"\"\n",
    "    Extracts 32-dimensional temporal features from the fc1 layer of the trained LSTM model.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the trained PyTorch model (.pth file).\n",
    "        X_test (np.ndarray): Preprocessed test sequences (NumPy array).\n",
    "        input_size (int): Number of features in each timestep.\n",
    "        hyperparams (dict): Dictionary of hyperparameters used to train the model.\n",
    "                            Expected keys: 'hidden_size', 'num_layers', 'dropout', 'activation'.\n",
    "                                           If None, default parameters will be used.\n",
    "    Returns:\n",
    "        np.ndarray: Extracted temporal features (batch_size, 32).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the model with the loaded hyperparameters\n",
    "    # Provide default values if hyperparams is None or a key is missing\n",
    "    model = EnhancedLSTM(\n",
    "        input_size=input_size,\n",
    "        hidden_size=hyperparams.get('hidden_size', 64),\n",
    "        num_layers=hyperparams.get('num_layers', 1),\n",
    "        dropout=hyperparams.get('dropout', 0.2),\n",
    "        activation=hyperparams.get('activation', 'relu')\n",
    "    )\n",
    "    \n",
    "    # Load the trained model's state dictionary\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    \n",
    "    # Convert numpy to tensor\n",
    "    X_tensor = torch.FloatTensor(X_test)\n",
    "    \n",
    "    # Extract features from the fc1 layer (32 dimensions)\n",
    "    features_list = []\n",
    "    \n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        # Process in batches to handle memory efficiently\n",
    "        batch_size = 32 # Can be adjusted\n",
    "        for i in range(0, len(X_tensor), batch_size):\n",
    "            batch = X_tensor[i:i+batch_size]\n",
    "            \n",
    "            # Forward pass through LSTM\n",
    "            lstm_out, (hn, cn) = model.lstm(batch)\n",
    "            last_output = lstm_out[:, -1, :] # Get the output from the last timestep\n",
    "            \n",
    "            # Get features from fc1 layer (before final prediction)\n",
    "            # Use model.activation for consistency with EnhancedLSTM\n",
    "            features = model.activation(model.fc1(last_output)) # 32-dimensional features\n",
    "            \n",
    "            features_list.append(features.numpy())\n",
    "    \n",
    "    # Combine all batches\n",
    "    all_features = np.vstack(features_list)\n",
    "    \n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65a90d43",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"EXTRACTING LSTM TEMPORAL FEATURES\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Purpose: Extract temporal features for LightGBM integration\")\n",
    "    print(\"Framework: Process -> LSTM Module -> Extract temporal features\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for prefix in DAY_PREFIXES:\n",
    "        print(f\"\\nProcessing {prefix}:\")\n",
    "        \n",
    "        # Define paths for optimized model and hyperparameters\n",
    "        optimized_model_path = f'{prefix}lstm_model_optimized.pth'\n",
    "        default_model_path = f'{prefix}lstm_model_default.pth' # Fallback for default training\n",
    "        hyperparams_path = f'{prefix}best_hyperparameters.json'\n",
    "        \n",
    "        model_to_load = None\n",
    "        hyperparams = {}\n",
    "\n",
    "        # Check if optimized model exists, otherwise check for default model\n",
    "        if os.path.exists(optimized_model_path):\n",
    "            model_to_load = optimized_model_path\n",
    "            print(f\"   Loading optimized model: {model_to_load}\")\n",
    "            if os.path.exists(hyperparams_path):\n",
    "                with open(hyperparams_path, 'r') as f:\n",
    "                    hyperparams = json.load(f)\n",
    "                print(f\"   Loaded hyperparameters: {hyperparams}\")\n",
    "            else:\n",
    "                print(f\"   Warning: Hyperparameters file not found for optimized model. Using default model params.\")\n",
    "                # Fallback to default params if JSON is missing for optimized model\n",
    "                hyperparams = {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.2, 'activation': 'relu'}\n",
    "        elif os.path.exists(default_model_path):\n",
    "            model_to_load = default_model_path\n",
    "            print(f\"   Loading default model: {model_to_load}\")\n",
    "            # For default model, use hardcoded default hyperparameters\n",
    "            hyperparams = {'hidden_size': 64, 'num_layers': 1, 'dropout': 0.2, 'activation': 'relu'}\n",
    "        else:\n",
    "            print(f\"   Error: Neither optimized model ({optimized_model_path}) nor default model ({default_model_path}) found.\")\n",
    "            print(f\"   Run lstm_model_enhanced_fixed.py first to train the model.\")\n",
    "            continue\n",
    "        \n",
    "        # Load preprocessed test data\n",
    "        npz_path = f'{prefix}lstm_preprocessed_data.npz'\n",
    "        if not os.path.exists(npz_path):\n",
    "            print(f\"   Preprocessed data not found: {npz_path}\")\n",
    "            print(f\"   Run lstm_preprocessing.py first\")\n",
    "            continue\n",
    "        \n",
    "        # Load the data\n",
    "        data = np.load(npz_path, allow_pickle=True)\n",
    "        X_test = data['X_test']\n",
    "        y_test = data['y_test']\n",
    "        \n",
    "        # Get input size from the data\n",
    "        input_size = X_test.shape[2] # Number of features\n",
    "        \n",
    "        print(f\"   Test data shape: {X_test.shape}\")\n",
    "        print(f\"   Input features: {input_size}\")\n",
    "        \n",
    "        try:\n",
    "            # Extract temporal features\n",
    "            print(f\"   Extracting temporal features...\")\n",
    "            temporal_features = extract_lstm_features_pytorch(model_to_load, X_test, input_size, hyperparams)\n",
    "            \n",
    "            # Save temporal features for LightGBM\n",
    "            feature_output_path = f'{prefix}lstm_temporal_features.npy'\n",
    "            np.save(feature_output_path, temporal_features)\n",
    "            \n",
    "            # Also save the corresponding targets\n",
    "            target_output_path = f'{prefix}lstm_targets.npy'\n",
    "            np.save(target_output_path, y_test)\n",
    "            \n",
    "            print(f\"   Features extracted: {temporal_features.shape}\")\n",
    "            print(f\"   Saved temporal features: {feature_output_path}\")\n",
    "            print(f\"   Saved targets: {target_output_path}\")\n",
    "            print(f\"   Feature dimensions: 32 (ready for LightGBM)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   Error extracting features: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"LSTM FEATURE EXTRACTION COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Temporal features ready for hybrid model integration\")\n",
    "    print(\"Next steps:\")\n",
    "    print(\"   1. Extract CapsNet spatial features\")\n",
    "    print(\"   2. Concatenate LSTM + CapsNet features\")\n",
    "    print(\"   3. Train LightGBM with combined features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98d82422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXTRACTING LSTM TEMPORAL FEATURES\n",
      "============================================================\n",
      "Purpose: Extract temporal features for LightGBM integration\n",
      "Framework: Process -> LSTM Module -> Extract temporal features\n",
      "============================================================\n",
      "\n",
      "Processing 11_10_:\n",
      "   Loading optimized model: 11_10_lstm_model_optimized.pth\n",
      "   Loaded hyperparameters: {'hidden_size': 256, 'num_layers': 1, 'dropout': 0.5, 'activation': 'tanh', 'learning_rate': 0.001, 'batch_size': 64, 'epochs': 50}\n",
      "   Test data shape: (23770, 60, 3)\n",
      "   Input features: 3\n",
      "   Extracting temporal features...\n",
      "   Features extracted: (23770, 32)\n",
      "   Saved temporal features: 11_10_lstm_temporal_features.npy\n",
      "   Saved targets: 11_10_lstm_targets.npy\n",
      "   Feature dimensions: 32 (ready for LightGBM)\n",
      "\n",
      "Processing 7_24_:\n",
      "   Loading optimized model: 7_24_lstm_model_optimized.pth\n",
      "   Loaded hyperparameters: {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.1, 'activation': 'tanh', 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 30}\n",
      "   Test data shape: (12864, 60, 1)\n",
      "   Input features: 1\n",
      "   Extracting temporal features...\n",
      "   Features extracted: (12864, 32)\n",
      "   Saved temporal features: 7_24_lstm_temporal_features.npy\n",
      "   Saved targets: 7_24_lstm_targets.npy\n",
      "   Feature dimensions: 32 (ready for LightGBM)\n",
      "\n",
      "Processing 10_19_:\n",
      "   Loading optimized model: 10_19_lstm_model_optimized.pth\n",
      "   Loaded hyperparameters: {'hidden_size': 64, 'num_layers': 3, 'dropout': 0.5, 'activation': 'tanh', 'learning_rate': 0.1, 'batch_size': 8, 'epochs': 50}\n",
      "   Test data shape: (24711, 60, 3)\n",
      "   Input features: 3\n",
      "   Extracting temporal features...\n",
      "   Features extracted: (24711, 32)\n",
      "   Saved temporal features: 10_19_lstm_temporal_features.npy\n",
      "   Saved targets: 10_19_lstm_targets.npy\n",
      "   Feature dimensions: 32 (ready for LightGBM)\n",
      "\n",
      "============================================================\n",
      "LSTM FEATURE EXTRACTION COMPLETE\n",
      "============================================================\n",
      "Temporal features ready for hybrid model integration\n",
      "Next steps:\n",
      "   1. Extract CapsNet spatial features\n",
      "   2. Concatenate LSTM + CapsNet features\n",
      "   3. Train LightGBM with combined features\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "        main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
